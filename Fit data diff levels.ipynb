{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "expe_name='12-13' \n",
    "\n",
    "CF=5000\n",
    "\n",
    "\n",
    "mode_CAP='C+R' #'R' #'C+R'\n",
    "\n",
    "E0_distributed=False #if True, E0 will be estimated from the main node of a distributed scheme (external process)\n",
    "#load params from E0_params.json\n",
    "Q10_distributed=False #if True, Q10 will be computed and estimated from the main node of a distributed scheme\n",
    "#load params from RBF_params.json\n",
    "I0_distributed=False #I0 for weibull cdf\n",
    "plus_lambda=False #if I0_distributed is True and plus_lambda is true, \n",
    "#the output for the RBF network for I0 corresponds to I0 + lambda (scale)\n",
    "load_wbcdf=False\n",
    "\n",
    "#if I0_distributed or load_wbcdf True , loads wb cdf params from results_folder0\n",
    "results_folder0=f'./results/fit{expe_name}-distrib/'  \n",
    "\n",
    "backend=dist.Backend('GLOO')\n",
    "n_workers=2\n",
    "rank=1\n",
    "\n",
    "filter_model='gammatone_4'  #'gaussian'\n",
    "\n",
    "load_json_optim_params=True #if True load optim params from optim_params.json\n",
    "load_json_init_params=True #if True, will load ./init_params/{expe_name}/{CF}_init_params.json if exists\n",
    "\n",
    "write_results=False #write ur, I/O func, Q10, lat params in files\n",
    "#to run (distributed): papermill -p E0_distributed True -p Q10_distributed True -p n_workers 5 -p rank 1 -p CF 4000 Fit\\ data.ipynb fitdata4000.ipynb\n",
    "\n",
    "sig_exc_plot=0.  #1.2 #gauss sigma for excitation patterns in time (in number of bins). for plots only  #0 if no filtering\n",
    "save_figs=False\n",
    "\n",
    "results_name=''  #if not blank, will save all the results in a folder with results_name (also loads param from this folder, like optim params)\n",
    "results_folder=None\n",
    "if results_name != '':\n",
    "    results_folder=f'./results/difflevels/fit{expe_name}-{results_name}/'\n",
    "\n",
    "    \n",
    "import_ur=False  #estimated as before by default, but for now these flags are overriden by imports and ur/weights are imported from standard expe\n",
    "import_weights=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_mode\n",
    "\n",
    "config_mode.init(mode_CAP)\n",
    "\n",
    "if expe_name=='12-13':\n",
    "    from fit_data_12_13_diff_levels import *\n",
    "    \n",
    "\n",
    "#pl.style.use('fivethirtyeight')\n",
    "\n",
    "pl.style.use('seaborn-deep')\n",
    "\n",
    "\n",
    "#mpl.rc('figure', figsize=(10,8))\n",
    "#mpl.rcParams['axes.facecolor']='white'  \n",
    "#mpl.rcParams['figure.facecolor'] = '1'\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from optim import *\n",
    "\n",
    "from rbf import RBFNet\n",
    "\n",
    "from suppression import *\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "if results_folder is None:\n",
    "    if Q10_distributed:\n",
    "        if I0_distributed:\n",
    "\n",
    "            results_folder=f'./results//difflevels/fit{expe_name}-distrib/I0_distrib/'\n",
    "        else:\n",
    "            results_folder=f'./results/difflevels/fit{expe_name}-distrib/'\n",
    "\n",
    "    else:\n",
    "        results_folder=f'./results/difflevels/fit{expe_name}/'\n",
    "\n",
    "if write_results:\n",
    "    print(f'writting results in {results_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common to all CFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_main_CAPs()\n",
    "plot_CAP_with_window()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat)\n",
    "#plotLatencies(lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list attenuations\n",
    "#print(capDataDic.keys())\n",
    "\n",
    "atten=40\n",
    "\n",
    "capData=capDataDic[atten]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First estimation of I/O masking curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_results and not(os.path.exists(results_folder)):\n",
    "    os.makedirs(results_folder)\n",
    "    \n",
    "\n",
    "def get_regexp(fln_list):\n",
    "    masker_list=[st.replace('-', '_').replace('.json', '') for st in fln_list]\n",
    "    reg_exp=')|('.join(masker_list)\n",
    "    reg_exp='('+reg_exp+')'\n",
    "    return reg_exp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the masking releases for the notched noise maskers with varying atten. for the notch. The amount of masking is evaluated as the reduction of the CAP peak-to-peak amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap=[]\n",
    "rms=[]\n",
    "masker_list=ntch_masker_lists[CF]  #, 'broadband_noise'   #note: maskers are not notched-noise maskers but I kept the same variable names as for previous expes\n",
    "masker_list=[st.replace('-', '_').replace('.json', '') for st in masker_list]\n",
    "\n",
    "\n",
    "fln_list=ntch_masker_lists[CF]\n",
    "reg_exp=ntch_regexps[CF]\n",
    "\n",
    "#ref_maskers=[capData.map_table_ref_maskers[fln] for fln in fln_list]\n",
    "#ref_masker_name=ref_maskers[0]\n",
    "#sig_ref=capData.get_signal_by_name(ref_masker_name)\n",
    "#assert ref_maskers.count(ref_masker_name)==len(ref_maskers)  #all conditions have same ref masker  #not true if including extra maskers\n",
    "\n",
    "noise_rms=0\n",
    "\n",
    "for i, masker in enumerate(masker_list):\n",
    "    if '20dB' in masker:\n",
    "        continue\n",
    "    if not(re.match(reg_exp, masker)):   #remove 'extra' maskers\n",
    "        continue\n",
    "    sig=capData.get_signal_by_name(masker, subtract_ref=True)\n",
    "    sig=process_signal(sig)\n",
    "    \n",
    "    cap_amp=np.max(sig)-np.min(sig)\n",
    "    #HACK\n",
    "    if ('17dB' in masker) or ('14dB' in masker):\n",
    "        cap_amp*=-1\n",
    "    cap.append(cap_amp)\n",
    "    #rms.append(np.std(sig-broadband_sig_trunc))\n",
    "    \n",
    "    #cap.append(np.max(sig)-np.min(sig))\n",
    "    #rms.append(np.std(sig-broadband_sig_trunc))\n",
    "    \n",
    "    \n",
    "    diff_sig_proc=gaussian_filter1d(sig, gauss_sigma) #noise amp computed on filtered version\n",
    "    \n",
    "\n",
    "    noise_rms+=np.mean(diff_sig_proc[:ind0]**2)\n",
    "    pl.plot(t2*1e3, diff_sig_proc*1e3, label=masker)\n",
    "     \n",
    "noise_rms=np.sqrt(noise_rms/len(masker_list))\n",
    "print(f'noise rms: {noise_rms*1e3:.3f} μV')\n",
    "\n",
    "pl.xlabel('t (ms)')\n",
    "pl.ylabel('Amplitude difference (μV)')\n",
    "\n",
    "pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "pl.show()\n",
    "\n",
    "pl.figure(figsize=(8, 6))\n",
    "\n",
    "attns=-attns_arrays[CF]\n",
    "pl.plot(attns+20, cap, '+', label='max-min')  #20  REF\n",
    "\n",
    "pl.legend()\n",
    "pl.xlabel('Notch attenuation')\n",
    "pl.ylabel('Amplitude difference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigm=SigmoidIOFunc(0, 0)\n",
    "#maskamount=1-(  (cap-np.amin(cap)) /np.amax(cap-np.amin(cap)) )\n",
    "maskamount=1-(cap/np.amax(cap)) \n",
    "\n",
    "I_pts=I0-atten+attns\n",
    "\n",
    "sigm.fit_data(I_pts, maskamount, constrained_at_Iref=True, Iref=I0-atten-20)\n",
    "\n",
    "wb_cdf=WeibullCDF_IOFunc()\n",
    "\n",
    "wb_cdf.fit_data(I_pts, maskamount, constrained_at_Iref=True, Iref=I0-atten-20)\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/maskamountCAP_{CF}.npz', I_pts=I_pts, maskamount=maskamount)\n",
    "    sigm.write_to_npz(f'{results_folder}/sigmIO_1st_estim_{CF}.npz')\n",
    "    wb_cdf.write_to_npz(f'{results_folder}/wbcfdIO_1st_estim_{CF}.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=np.linspace(-30, 25)\n",
    "\n",
    "fig=pl.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "pl.plot(I, sigm(torch.tensor(I))*100, label='fit sigmoid')\n",
    "pl.plot(I, wb_cdf(torch.tensor(I))*100, label='fit Weibull CDF')\n",
    "\n",
    "#plot after optim\n",
    "#pl.plot(I, wb_cdf2(torch.tensor(I)).clone().detach().numpy()*100, label='   (after optim.)', color='C1', linestyle='--')\n",
    "\n",
    "pl.xlabel('Power spectral density (dB)')\n",
    "\n",
    "pl.plot(I_pts, maskamount*100, '+', markersize=10, markeredgewidth=3, label='based on ΔCAP \\namplitude')\n",
    "\n",
    "pl.plot(I0-atten-20, 100, '+', markersize=10, markeredgewidth=3, color='purple')\n",
    "\n",
    "pl.ylabel('Masking (%)')\n",
    "\n",
    "#pl.xlim([-25, 17])\n",
    "pl.ylim([0, 130])\n",
    "\n",
    "for key, spine in ax.spines.items():\n",
    "    spine.set_visible(True)\n",
    "    \n",
    "    spine.set_linewidth(1.3)\n",
    "    spine.set_edgecolor('black')\n",
    "\n",
    "\n",
    "ax.grid(which='minor')\n",
    "\n",
    "\n",
    "pl.legend()\n",
    "#pl.savefig('IO_func_fit.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting model/Defining releases of masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reg_exp=ntch_regexps[CF]  #previous method\n",
    "\n",
    "\n",
    "ntch_maskerNames, ntch_maskingConds, ntch_signals =capData.get_batch_re(reg_exp, subtract_ref=True)\n",
    "ntch_maskingConds.set_amp0_dB(I0-atten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HACK pad maskers >12e3 to avoid issues with latencies (equivalent to taking the difference\n",
    "#  excitations of maskers - excitation 'broadband noise')\n",
    "ntch_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)\n",
    "#ntch_maskingConds.pad_maskers2() #no need to do the same for low freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gauss_sigma=(1e-4)/(t2[1]-t2[0])  #gauss_sigma defined in common.py\n",
    "ntch_signals_proc=process_signal2(ntch_signals, gauss_sigma=gauss_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "for maskerName, sig in zip(ntch_maskerNames, ntch_signals_proc):\n",
    "    pl.plot(t2, sig, label=maskerName)\n",
    "\n",
    "pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation ur\n",
    "\n",
    "sig=capData.get_signal_by_name('3_hp6000_narrowband5kHz_35dB', subtract_ref=True)\n",
    "\n",
    "\n",
    "#sig2=process_signal(sig)\n",
    "#pl.plot((t2-3e-3)*1e3, sig2-broadband_proc)\n",
    "#pl.plot(t2*1e3, sig2bis-broadband_proc)\n",
    "\n",
    "\n",
    "\n",
    "sig2=process_signal2(sig, gauss_sigma=gauss_sigma)\n",
    "pl.plot((t2-3e-3)*1e3, sig2)\n",
    "\n",
    "\n",
    "\n",
    "t_shift=5.7e-3-3e-3 #excitation coincides with CM\n",
    "\n",
    "#ur0=sig2-broadband_proc\n",
    "ur0=sig2\n",
    "ur0=np.roll(ur0,  -int(t_shift*48828) )\n",
    "pl.plot((t2-3e-3)*1e3, ur0)\n",
    "#pl.xlim([4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_model=lat  #arleady shifted?\n",
    "\n",
    "#ref click\n",
    "#lat_shifted=PowerLawLatencies.shift(lat_model, 4.8e-3-1e-3) #t0: (peak deconv ref:click)  + (t_click-1ms)\n",
    "\n",
    "#lat_shifted.name='true latencies'\n",
    "\n",
    "use_bincount=True\n",
    "#if use_bincount:\n",
    "#    lat_model=lat_shifted\n",
    "#else:\n",
    "#    #HACK as latencies are very small (sampling issues), manual dilatation\n",
    "#    lat_model=PowerLawLatencies.fromPts(0.0056, 10000, 0.007, 800, name= 'dilatated (hack)')\n",
    "#    #not required with bincount\n",
    "#    #lat_model=PowerLawLatencies.fromPts(0.0057, 9500, 0.0062, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test single lat model\n",
    "singleLat=False\n",
    "if singleLat or use_bincount:\n",
    "    if CF>6500:\n",
    "        f_min=4000\n",
    "        f_max=12000\n",
    "    elif CF < 5500:\n",
    "        if CF<4500:\n",
    "            f_min=600\n",
    "            f_max=7500\n",
    "        else:\n",
    "            f_min=1000\n",
    "            f_max=10000\n",
    "    else:\n",
    "        f_min=2500\n",
    "        f_max=9000\n",
    "        \n",
    "    if E0_distributed:\n",
    "        with open('E0_params.json') as f:\n",
    "            params = json.load(f)\n",
    "            f_min=float(params['f_min'])\n",
    "            f_max=float(params['f_max'])\n",
    "        \n",
    "    if singleLat:\n",
    "        lat_model = SingleLatency(6e-3, f_min=f_min, f_max=f_max)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if singleLat or use_bincount:\n",
    "    \n",
    "    if E0_distributed:\n",
    "        with open('E0_params.json') as f:\n",
    "            params = json.load(f)\n",
    "            m=int(params['m'])\n",
    "    else:\n",
    "        m=400\n",
    "    E0=1/2*np.ones((m,))\n",
    "    \n",
    "    \n",
    "    f=np.linspace(f_min, f_max, m)\n",
    "    \n",
    "    if import_weights:\n",
    "        E0=weights_E0_amp*np.interp(f, weights_f, weights_E0)\n",
    "    \n",
    "    pl.plot(f, E0)\n",
    "    pl.xlabel('Frequency (kHz)')\n",
    "    pl.ylabel('Init raw excitation')\n",
    "    \n",
    "    #ind for CF (can be useful later)\n",
    "    ind_CF=int((CF-f_min)/(f_max-f_min)*m)\n",
    "else:\n",
    "    m=72\n",
    "    E0_temp=sg.windows.tukey(m, alpha=0.5) \n",
    "    E0=np.zeros_like(t2)\n",
    "    ind_begin=int((t_shift-1e-3)*48828)\n",
    "    ind_end=int((t_shift-1e-3)*48828)+m\n",
    "    E0[ind_begin:ind_end]=E0_temp\n",
    "\n",
    "    pl.plot(t2*1e3, E0)\n",
    "    pl.title('Init raw excitation')\n",
    "    pl.xlabel('t (ms)')\n",
    "    pl.ylabel('Amp')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plotLatencies(lat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=ExcitationPatterns(t2, E0, use_bincount=use_bincount, bincount_fmin=f_min, bincount_fmax=f_max)  #no non-maskable part\n",
    "\n",
    "#NB: first model for estimation of ur, cte bandwith, fixed\n",
    "\n",
    "Q_10_0=2*(CF/1000)**0.5\n",
    "BW10_0=CF/Q_10_0\n",
    "\n",
    "#BW10_0Func=constant_BW10(BW10_0, requires_grad=False)   #constant BW\n",
    "\n",
    "#Q10 defined by the power law above\n",
    "BW10_0Func=Q10PowerLaw(2, 1000, 0.5, requires_grad=False)  \n",
    "\n",
    "print(f'BW10 for first guess: {BW10_0Func(CF):.1f} Hz')\n",
    "\n",
    "#E.set_masking_model(lat_model, BW10_0Func, ntch_maskingConds, sigm, filter_model=filter_model)\n",
    "E.set_masking_model(lat_model, BW10_0Func, ntch_maskingConds, wb_cdf, filter_model=filter_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimation of unitary response**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO clean\n",
    "\n",
    "The unitary response is estimated by deconvolution of the CAP masking releases $[\\Delta CAP(t)]_i$ for the notched-noise maskers with varying attenuation for the notch. A first guess for the masking release patterns is used (after optimization of the model parameters, a re-estimation of ur can be done with `load_wbcdf=True`, before a second optimisation). The UR is taken as the average of the deconvolved signals, weighted by the quadratic sum of $[\\Delta CAP(t)]_i$ for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signals from which ur is estimated\n",
    "#all notched noise maskers around CF?\n",
    "\n",
    "fln_list=ntch_masker_lists[CF]\n",
    "\n",
    "reg_exp=get_regexp(fln_list)\n",
    "\n",
    "ur_estim_maskerNames, ur_estim_maskingConds, ur_estim_signals =capData.get_batch_re(reg_exp, subtract_ref=True)\n",
    "ur_estim_maskingConds.set_amp0_dB(I0-atten)\n",
    "\n",
    "gauss_sigma_deconv=2*gauss_sigma \n",
    "\n",
    "ur_estim_signals_proc=process_signal2(ur_estim_signals, gauss_sigma=gauss_sigma_deconv)\n",
    "ur_estim_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ur_estim_maskingConds.mat_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimation ur\n",
    "\n",
    "\n",
    "if load_wbcdf or I0_distributed:\n",
    "    wb_cdf=WeibullCDF_IOFunc.load_from_npz(f'{results_folder0}/wbcfdIO_{CF}.npz')\n",
    "\n",
    "E.set_masking_model(lat_model, BW10_0Func, ur_estim_maskingConds, wb_cdf, filter_model=filter_model)\n",
    "\n",
    "maskAmounts, excs = E.get_tensors() \n",
    "\n",
    "nb_steps=20\n",
    "alpha=np.linspace(0.5, 0.05, nb_steps)\n",
    "\n",
    "EPs_fft=np.fft.rfft(excs, axis=1)\n",
    "CAPs_fft=np.fft.rfft(ur_estim_signals_proc, axis=1)\n",
    "#u1_mat=np.tile(ur0, (ur_estim_maskingConds.n_conditions, 1))\n",
    "u1_mat=np.zeros_like(ur_estim_signals_proc)\n",
    "#filter_mat  = (t2>7.5e-3)+(t2<3.2e-3)\n",
    "filter_mat  = (t2>7.5e-3)\n",
    "filter_mat=np.tile(filter_mat, (ur_estim_maskingConds.n_conditions, 1))\n",
    "#filter_mat=np.zeros_like(ntch_signals_proc, dtype=bool)\n",
    "#proj_fft=E.get_projector_fft()\n",
    "\n",
    "weights=np.sqrt(np.sum(excs.clone().detach().numpy()**2, axis=1))\n",
    "for i in range(1, nb_steps+1):\n",
    "    du=deconv_newton_step(u1_mat, EPs_fft, CAPs_fft, eps_ridge=0)   \n",
    "    #du=deconv_grad(u1_mat, EPs_fft, CAPs_fft)\n",
    "    \n",
    "    u1_mat-=alpha[i-1]*du\n",
    "    #proj 1 \n",
    "    u1_mat[filter_mat]=np.zeros_like(u1_mat[filter_mat])\n",
    "    #proj 2\n",
    "\n",
    "    #u1_mat_mean=np.mean(u1_mat, axis=0)[None, :]\n",
    "    \n",
    "    \n",
    "    #weighted average\n",
    "    u1_mat_mean=np.average(u1_mat, axis=0, weights=weights)[None, :]\n",
    "    u1_mat=np.repeat(u1_mat_mean, ur_estim_maskingConds.n_conditions, axis=0)\n",
    "     \n",
    "    '''\n",
    "    for i in range(5):\n",
    "            pl.figure()\n",
    "            name=ntch_maskerNames[i]\n",
    "            pl.plot(u1_mat[i], label=name, color=f'C{i}')\n",
    "            #pl.plot( np.abs(EPs_fft[i]))\n",
    "            pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    '''\n",
    "    \n",
    "\n",
    "    if i==nb_steps:\n",
    "        pl.figure()\n",
    "        pl.title(f'Step {i} (deconv + proj)')\n",
    "        #pl.plot(t, u0, label='u0 (truth)')\n",
    "        #pl.plot(t2, u_temp, label='u0 (last save)')\n",
    "        pl.plot(t2, u1_mat[0], label='u0 (estimated)')\n",
    "        pl.legend()\n",
    "        #pl.savefig('ur_8kHz_Q395.svg')\n",
    "        pl.show()\n",
    "        u_temp=u1_mat[0]\n",
    "#if write_results:\n",
    "#    np.savez(f'{results_folder}/ur_{CF}.npz', t2=t2, ur=u1_mat[0])\n",
    "#saved after normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maskers and signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verious freqs. (for estimating E0)\n",
    "if not import_weights:\n",
    "    fln_list=vfreq_fln_lists[CF]\n",
    "\n",
    "    reg_exp=get_regexp(fln_list)\n",
    "\n",
    "    vfreq_maskerNames, vfreq_maskingConds, vfreq_signals =capData.get_batch_re(reg_exp, subtract_ref=True)\n",
    "    vfreq_signals_proc=process_signal2(vfreq_signals, gauss_sigma=gauss_sigma)\n",
    "    vfreq_maskingConds.set_amp0_dB(I0-atten)\n",
    "    vfreq_maskingConds.pad_maskers(f_thr=11000, f_max=np.Inf)\n",
    "\n",
    "#various bws (for estimating Q10)\n",
    "fln_list=vbw_fln_lists[CF]\n",
    "\n",
    "reg_exp=get_regexp(fln_list)\n",
    "\n",
    "vbw_maskerNames, vbw_maskingConds, vbw_signals =capData.get_batch_re(reg_exp, subtract_ref=True)\n",
    "vbw_signals_proc=process_signal2(vbw_signals, gauss_sigma=gauss_sigma)\n",
    "vbw_maskingConds.set_amp0_dB(I0-atten)\n",
    "#HACK\n",
    "vbw_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_freq_factor=1.14 #(Altoe et al 2021)\n",
    "\n",
    "supp_BW10Func=Q10PowerLaw(0.52, 1000, 0.58, requires_grad=False)   #10kHz -> 3.8   np.log10(3.8)=0.57978\n",
    "#Charaziak and Siegel 2014\n",
    "\n",
    "#suppression amount\n",
    "suppFunc=LogLinearSuppression(0.5, -30., requires_grad=True)  #0.5 #0.2\n",
    "suppAmount=SuppressionAmount(suppFunc, supp_BW10Func, supp_freq_factor)\n",
    "\n",
    "#if no suppression\n",
    "#suppAmount=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning of model parameters using gradient descent (I/O curve, Q10, frequency weights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try more accurate estimation of i/o curve\n",
    "\n",
    "signals_proc=ntch_signals_proc\n",
    "maskingConds=ntch_maskingConds\n",
    "\n",
    "\n",
    "io_func = 'weibull' \n",
    "#io_func= 'sigm'\n",
    "\n",
    "#sigm2=SigmoidIOFunc(sigm.mu.numpy(), sigm.a.numpy(), Iref=I0-20, constrained_at_Iref=True, requires_grad=True)\n",
    "\n",
    "\n",
    "#sigm2=SigmoidIOFunc(sigm.mu.numpy(), sigm.a.numpy(), Iref=I0-20, constrained_at_Iref=True, requires_grad=True)\n",
    "\n",
    "sigm2=SigmoidIOFunc(5., 0.25, Iref=I0-20, constrained_at_Iref=True, requires_grad=True)\n",
    "\n",
    "init_params_json=f'./init_params/{expe_name}_difflevels/{CF}_init_params.json'\n",
    "if load_json_init_params and os.path.exists(init_params_json):\n",
    "    \n",
    "    with open(init_params_json) as f:\n",
    "        dic_params=json.load(f)\n",
    "        k_cdf=float(dic_params['k'])\n",
    "        I0_cdf=float(dic_params['I0'])\n",
    "        scale_cdf=float(dic_params['scale'])\n",
    "else:\n",
    "    if expe_name=='12-13':\n",
    "        #I0_cdf=-28.-3+30\n",
    "        #k_cdf=3.5\n",
    "        #scale_cdf=37.\n",
    "        \n",
    "        #with Isupp(0.2dB/dB, I0supp=-30)\n",
    "        #I0_cdf=-34.+30.\n",
    "        #k_cdf=3.5\n",
    "        #scale_cdf=35.\n",
    "        \n",
    "        #with Isupp(0.5dB/dB, I0supp=-30)\n",
    "        I0_cdf=-39.+30.\n",
    "        k_cdf=5.5\n",
    "        scale_cdf=27.\n",
    "        \n",
    "        #with Isupp(0.6dB/dB, I0supp=-30)\n",
    "        #I0_cdf=-40.+30.\n",
    "        #k_cdf=7.\n",
    "        #scale_cdf=24.\n",
    "        \n",
    "        \n",
    "        #E.E0_maskable_amp.data=torch.tensor(4.)\n",
    "    else:\n",
    "        I0_cdf=-20.\n",
    "        k_cdf=5.\n",
    "        scale_cdf=30.\n",
    "    \n",
    "I0_cdf-=atten\n",
    "    \n",
    "#copy first wb cdf (from fit points)\n",
    "'''\n",
    "wb_cdf2=WeibullCDF_IOFunc(I0=wb_cdf.I0.data,\n",
    "    scale=wb_cdf.scale.data,\n",
    "    k=wb_cdf.k.data,\n",
    "    mmax=1.,\n",
    "    requires_grad=True,\n",
    "    constrained_at_Iref=True,\n",
    "    Iref=I0-atten-20)\n",
    "'''    \n",
    "\n",
    "wb_cdf2=WeibullCDF_IOFunc(I0=I0_cdf,\n",
    "    scale=scale_cdf,\n",
    "    k=k_cdf,\n",
    "    mmax=1.,\n",
    "    requires_grad=True,\n",
    "    constrained_at_Iref=True,\n",
    "    Iref=I0-atten-20-3)\n",
    "\n",
    "\n",
    "#E2=ExcitationPatterns(t2, E0, requires_grad=True)  #no non-maskable part\n",
    "E2=ExcitationPatterns.copyRaw(E, requires_grad=True)\n",
    "if Q10_distributed or E0_distributed:   \n",
    "    #init group\n",
    "    if not(dist.is_initialized()):\n",
    "        dist.init_process_group(backend, init_method='tcp://127.0.0.1:1234', world_size=n_workers, rank=rank, \n",
    "                                timeout=datetime.timedelta(0, 80))  \n",
    "    \n",
    "    if Q10_distributed:\n",
    "        Q10rbf=Q10RBFNet.create_from_jsonfile('RBF_params.json')\n",
    "        #update weights (have to be sent by main process)\n",
    "        Q10rbf.update_weights()\n",
    "        BW10_0TestFunc=Q10RBFNet_BW10(Q10rbf)\n",
    "    #Not needed to load weights for E0 as should be initialized at 1 anyway\n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    BW10_0TestFunc=constant_BW10(BW10_0, requires_grad=True)\n",
    "    \n",
    "    \n",
    "if I0_distributed:\n",
    "    I0_rbf=RBFNet.create_from_jsonfile('RBF_I0_params.json')\n",
    "    wb_cdf2.set_I0_w_RBFNet(I0_rbf, plus_lambda=plus_lambda)\n",
    "    #update weights (have to be sent by main process)\n",
    "    I0_rbf.update_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim params\n",
    "#NOTE: params not imported from json file for first optim(?)\n",
    "\n",
    "alpha=1\n",
    "alpha_Q10=3e7\n",
    "\n",
    "#for estimation of E0\n",
    "n_dim=7 #projection of gradient on n_dim first harmomics (Fourier basis)\n",
    "\n",
    "\n",
    "if io_func=='weibull':\n",
    "    #alpha_dic={wb_cdf2.I0: 30*alpha, wb_cdf2.scale: 5*alpha, wb_cdf2.k: 30*alpha}\n",
    "    alpha_dic={wb_cdf2.scale: alpha, wb_cdf2.k: 10*alpha}\n",
    "    \n",
    "    if I0_distributed:\n",
    "        alpha_dic[wb_cdf2.rbfNet.l2.weight]=0.005*alpha\n",
    "    else:\n",
    "        alpha_dic[wb_cdf2.I0]=10*alpha\n",
    "        \n",
    "    #alpha_dic={wb_cdf2.I0: 0.1*alpha, wb_cdf2.scale: 0.05*alpha, wb_cdf2.k: 0.6*alpha}\n",
    "else:\n",
    "    alpha_dic={sigm2.mu: 0.01*alpha, sigm2.a: 0.005*alpha}\n",
    "\n",
    "#alpha_dic[BW10_0TestFunc.BW_10]=alpha\n",
    "#alpha_dic[E2.E0_maskable]=0.1*alpha  #/!| with sum_grad_E0 set to True #previous method to modify E0 amp\n",
    "alpha_dic[E2.E0_maskable_amp]=0.1*alpha  \n",
    "\n",
    "\n",
    "alpha_dic_Q10={}\n",
    "\n",
    "if Q10_distributed:\n",
    "    alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]=0.05*alpha\n",
    "else:\n",
    "    alpha_dic_Q10[BW10_0TestFunc.BW_10]=alpha_Q10 #cte bw\n",
    "    \n",
    "\n",
    "\n",
    "alpha_dic_E0={E2.E0_maskable: 0.2*alpha}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first loop of 50 gradient descent steps is performed to fit the I/O function on the responses for the notched-noise maskers with a varying attenuation for the notch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first optim I/O func (if not I0 distributed or load_wbcdf)\n",
    "\n",
    "nb_stepsIO=50\n",
    "\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, \n",
    "                         filter_model=filter_model, suppression_model=suppAmount)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model,\n",
    "                        suppression_model=suppAmount)\n",
    "\n",
    "if not(I0_distributed or load_wbcdf):\n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "                nb_steps=nb_stepsIO, #sum_grad_E0=True, \n",
    "                                plot_masking_I0_graph=True,\n",
    "               step_plots=5)\n",
    "else:\n",
    "    #import params (k, scale)\n",
    "    \n",
    "    wb_cdf_temp=WeibullCDF_IOFunc.load_from_npz(f'{results_folder0}/wbcfdIO_{CF}.npz')\n",
    "    if not(I0_distributed):\n",
    "        wb_cdf2.I0.data=wb_cdf_temp.I0.data\n",
    "    wb_cdf2.k.data=wb_cdf_temp.k.data\n",
    "    wb_cdf2.scale.data=wb_cdf_temp.scale.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unitary response is normalized (so that it is comparable across CFs, i.e. different optimization nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention: normalize so that amp=0.0001 (/20 factor compared to others expes)\n",
    "#norm_factor=0.0001/(np.amax(u1_mat[0])-np.amin(u1_mat[0]) ) #normalization peak-to-peak\n",
    "norm_factor=0.00005/(-np.amin(u1_mat[0]) ) #normalization N1\n",
    "\n",
    "u1_mat*=norm_factor\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/ur_{CF}.npz', t2=t2, ur=u1_mat[0])\n",
    "\n",
    "if not(E0_distributed):\n",
    "    E2.E0_maskable.data=E2.E0_maskable/norm_factor\n",
    "else:\n",
    "    #send norm_factor to main node\n",
    "    send_norm_factor_hand=dist.isend(torch.tensor(norm_factor, dtype=torch.float64),0, tag=99)\n",
    "    send_norm_factor_hand.wait() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Params for main loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note change params for I/O func\n",
    "#TODO write in json?\n",
    "#loads params from json file\n",
    "if io_func=='weibull':\n",
    "    if not(I0_distributed):\n",
    "        alpha_dic[wb_cdf2.I0]=0.5*alpha\n",
    "    alpha_dic[wb_cdf2.scale]= 0.05*alpha\n",
    "    alpha_dic[wb_cdf2.k]= 0.5*alpha    \n",
    "\n",
    "n_it=100  #100\n",
    "nb_steps=3 #5\n",
    "\n",
    "\n",
    "if load_json_optim_params:\n",
    "    if os.path.exists(f'optim_params_{expe_name}_difflevels.json'):\n",
    "        optim_params_filename=f'optim_params_{expe_name}_difflevels.json'\n",
    "    else:\n",
    "        optim_params_filename='optim_params.json'\n",
    "    with open(optim_params_filename) as f:\n",
    "        dic_params=json.load(f)\n",
    "    \n",
    "    n_it=dic_params['n_it']\n",
    "    nb_steps=dic_params['nb_steps']\n",
    "    n_dim=dic_params['n_dim']\n",
    "    step_values=dic_params['alpha']\n",
    "    if io_func=='weibull':\n",
    "        if I0_distributed:\n",
    "            alpha_dic[wb_cdf2.rbfNet.l2.weight]=float(step_values['I0RBFweights'])\n",
    "        else:\n",
    "            alpha_dic[wb_cdf2.I0]=float(step_values['I0'])\n",
    "        alpha_dic[wb_cdf2.scale]= float(step_values['scale'])\n",
    "        alpha_dic[wb_cdf2.k]= float(step_values['k'] )\n",
    "    else:\n",
    "        alpha_dic[sigm2.mu]= float(step_values['sigm_mu'])\n",
    "        alpha_dic[sigm2.a]= float(step_values['sigm_a'])\n",
    "    alpha_dic[E2.E0_maskable_amp]=float(step_values['E0_amp'])\n",
    "        \n",
    "\n",
    "    if Q10_distributed:\n",
    "        alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]=float(step_values['Q10RBFweights'])\n",
    "    else:\n",
    "        alpha_dic_Q10[BW10_0TestFunc.BW_10]=float(step_values['Q10']) #cte bw\n",
    "                                  \n",
    "    \n",
    "    alpha_dic_E0[E2.E0_maskable]=float(step_values['E0'])\n",
    "\n",
    "    \n",
    "alpha_dic_Q10[suppFunc.a]=0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optim (main loop)**\n",
    "\n",
    "The model parameters are fine-tuned with an alternate gradient scheme.\n",
    " 1. Update of the weights R0 (E0 in the code). Notched noise maskers with a notch present on a broad range of frequencies around CF are used for the computation of gradients. 3 steps\n",
    " 2.  Update of the weights I/0 function (computation of gradients over: notched noise maskers with varying attenuation at the notch. notch centered at CF only). 3 steps\n",
    " 3.  Update of Q_10 (notched noise maskers with varying notch width. Maskers with CF belonging to the notch only)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Debug plot gradients\n",
    "\n",
    "E2.E0_maskable_amp.data=E0[ind_CF]/E2.E0_maskable[ind_CF]*1/norm_factor\n",
    "\n",
    "pl.figure(figsize=(6, 4))\n",
    "\n",
    "#I/O Func (+ amp E0)\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "            nb_steps=1, #sum_grad_E0=True, \n",
    "            plot_E0_graph=False, plot_masking_I0_graph=False,\n",
    "            plot_Q10=False, debug_grad_excs=True,\n",
    "                                       fc_ref_Q10=CF, \n",
    "           step_plots=1) #axes=axes, ind_plots=ind_plots, step0=(3*i+1)*nb_steps, tot_steps=tot_steps, I0_distributed=I0_distributed\n",
    "pl.xlim([6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tot_steps=3*n_it*nb_steps\n",
    "\n",
    "errs=[]\n",
    "errs_total=[]\n",
    "\n",
    "pl.figure(figsize=(6, 12))\n",
    "\n",
    "for i in range(n_it):\n",
    "    \n",
    "    if Q10_distributed or E0_distributed or I0_distributed: #informs the main node that optim is still in progress\n",
    "        optim_done_hand=dist.isend(torch.tensor(nb_steps, dtype=torch.int32),0, tag=16)\n",
    "        optim_done_hand.wait()\n",
    "    \n",
    "    if E0_distributed:  #update E0\n",
    "        hand = dist.irecv(E2.E0_maskable, src=0, tag=8)\n",
    "        hand.wait()\n",
    "    if Q10_distributed:\n",
    "        Q10rbf.update_weights()\n",
    "    \n",
    "    if I0_distributed:\n",
    "        I0_rbf.update_weights()\n",
    "       \n",
    "    \n",
    "    if i==0:\n",
    "        axes=None\n",
    "        ind_plots=None\n",
    "        \n",
    "        \n",
    "    #E0\n",
    "    if i==0:\n",
    "\n",
    "         #update E0_amp if E0_distributed for a few steps before anything else\n",
    "        if E0_distributed:\n",
    "                    #HACK\n",
    "            #try to have E0_amp and E0 values consistent with each other\n",
    "            E2.E0_maskable_amp.data=E0[ind_CF]/E2.E0_maskable[ind_CF]*1/norm_factor\n",
    "\n",
    "        \n",
    "            if io_func=='weibull':\n",
    "                E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, \n",
    "                                     filter_model=filter_model, suppression_model=suppAmount)\n",
    "            else:\n",
    "                E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, \n",
    "                                     filter_model=filter_model, suppression_model=suppAmount)\n",
    "    \n",
    "            optim_steps(E2, u1_mat[0], signals_proc, {E2.E0_maskable_amp:alpha_dic[E2.E0_maskable_amp]}, \n",
    "                nb_steps=nb_steps*5)\n",
    "    \n",
    "    if not import_weights:\n",
    "        if io_func=='weibull':\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "        else:\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "\n",
    "\n",
    "\n",
    "        axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], vfreq_signals_proc, alpha_dic_E0, \n",
    "            nb_steps=nb_steps, \n",
    "            n_dim_E0=n_dim, \n",
    "            E0_distributed=E0_distributed,                        \n",
    "             #E0_t_min=t_min_E0, E0_t_max=t_max_E0, k_mode_E0=k_mode_E0,\n",
    "            plot_E0_graph=True, plot_E0_amp_graph=True, plot_masking_I0_graph=True,\n",
    "            plot_Q10=True, fc_ref_Q10=CF, step_plots=5, axes=axes, ind_plots=ind_plots, \n",
    "            step0=(3*i)*nb_steps, tot_steps=tot_steps) \n",
    "        \n",
    "        err0=err_list[-1]\n",
    "\n",
    "    #I/O Func (+ amp E0)\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    \n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "                nb_steps=nb_steps, #sum_grad_E0=True, \n",
    "                plot_E0_graph=True, plot_masking_I0_graph=True,\n",
    "                plot_Q10=True, plot_supp_params=True,\n",
    "                fc_ref_Q10=CF,\n",
    "               step_plots=5, axes=axes, ind_plots=ind_plots, step0=(3*i+1)*nb_steps,\n",
    "                 tot_steps=tot_steps, I0_distributed=I0_distributed)\n",
    "    err1=err_list[-1]\n",
    "\n",
    "#     #Q10\n",
    "    \n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "\n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], vbw_signals_proc, alpha_dic_Q10, \n",
    "            nb_steps=nb_steps, sum_grad_E0=True, \n",
    "            plot_E0_graph=True, plot_masking_I0_graph=True, plot_supp_params=True,\n",
    "            plot_Q10=True, fc_ref_Q10=CF, \n",
    "           step_plots=5, axes=axes, ind_plots=ind_plots, step0=(3*i+2)*nb_steps,\n",
    "               tot_steps=tot_steps, #verbose=i%5,\n",
    "               Q10_distributed=Q10_distributed)\n",
    "    err2=err_list[-1]\n",
    "    err_sum=(err1+err2)\n",
    "    errs.append(err_sum.detach().numpy())  #errors are summed only on notched noise maskers (update I/O curve and Q10)\n",
    "    #nb: possible duplicates\n",
    "    \n",
    "    if not import_weights:\n",
    "        errs_total.append( (err0+err1+err2))\n",
    "\n",
    "    \n",
    "if Q10_distributed or E0_distributed or I0_distributed: #informs the main node that optim is done\n",
    "    optim_done_hand=dist.isend(torch.tensor(0, dtype=torch.int32),0, tag=16)\n",
    "    #optim_done_hand.wait()\n",
    "        \n",
    "pl.tight_layout()\n",
    "if save_figs:\n",
    "    pl.savefig(f'fitdata{CF}_optim_steps.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_2=np.sum(vbw_signals_proc**2)+np.sum(ntch_signals_proc**2)  \n",
    "if not import_weights:\n",
    "    rms_2b=rms_2+np.sum(vfreq_signals_proc**2)  \n",
    "\n",
    "pl.figure()\n",
    "pl.plot(np.arange(len(errs)), errs/rms_2*100, label='notched noise maskers')\n",
    "\n",
    "#pl.plot(np.arange(len(errs)), errs_total/rms_2*100, label='all maskers (w/ duplicates)')\n",
    "\n",
    "pl.xlabel('Iterations')\n",
    "pl.ylabel('Error (% variance)')\n",
    "\n",
    "pl.legend()\n",
    "\n",
    "if save_figs:\n",
    "    pl.savefig(f'fitdata{CF}_optim_steps_err.svg')\n",
    "pl.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data\n",
    "if write_results:\n",
    "\n",
    "    \n",
    "    \n",
    "    if io_func=='weibull':\n",
    "        if I0_distributed:\n",
    "            I0_=wb_cdf2.rbfNet(torch.tensor([CF]))\n",
    "            if plus_lambda:\n",
    "                I0_-=wb_cdf2.scale\n",
    "            wb_cdf2.I0=I0_[0]\n",
    "        wb_cdf2.write_to_npz(f'{results_folder}/wbcfdIO_{CF}.npz')\n",
    "    else:\n",
    "        sigm2.write_to_npz(f'{results_folder}/sigmIO_{CF}.npz')    \n",
    "    \n",
    "    if isinstance(lat_model, SingleLatency):\n",
    "        np.savez(f'{results_folder}/E0_{CF}.npz', f=lat_model.get_f_linspace(len(t2)).detach().numpy(),\n",
    "                E0=E2.E0_maskable.detach().numpy(), lat=lat_model.t0, E0_amp=E2.E0_maskable_amp.detach().numpy())\n",
    "    else:\n",
    "        if use_bincount: \n",
    "            np.savez(f'{results_folder}/E0_{CF}.npz', f=E2.bincount_f.detach().numpy(),\n",
    "                E0=E2.E0_maskable.detach().numpy(), E0_amp=E2.E0_maskable_amp.detach().numpy())\n",
    "            \n",
    "       \n",
    "        #save lat model\n",
    "        lat_model.write_to_npz(f'{results_folder}/lat_{CF}.npz') #Note: normally lat does not depend on CF but it could\n",
    "        \n",
    "    if Q10_distributed:\n",
    "        pass\n",
    "\n",
    "    Q10optim= CF/E2.bw10Func(torch.tensor(CF, dtype=torch.float32))\n",
    "    np.save(f'{results_folder}/Q10optim_{CF}.npy',\n",
    "            Q10optim.detach().numpy() )\n",
    "    \n",
    "    \n",
    "    #write params\n",
    "    \n",
    "    json_data={}\n",
    "    json_data[\"n_it\"]=n_it\n",
    "    json_data[\"nb_steps\"]=nb_steps\n",
    "    json_data[\"tot_steps\"]=tot_steps\n",
    "\n",
    "    alpha=30\n",
    "    alpha_Q10=3e7\n",
    "\n",
    "\n",
    "    #for estimation of E0\n",
    "    json_data[\"n_dim\"]=n_dim \n",
    "\n",
    "    if io_func=='weibull':\n",
    "        json_data_alpha={\"scale\": alpha_dic[wb_cdf2.scale],\n",
    "                         \"k\": alpha_dic[wb_cdf2.k]}\n",
    "        if I0_distributed:\n",
    "            json_data_alpha[\"I0_rbf_weights\"]=alpha_dic[wb_cdf2.rbfNet.l2.weight]\n",
    "        else:\n",
    "            json_data_alpha[\"I0\"]=alpha_dic[wb_cdf2.I0]\n",
    "    else:\n",
    "        json_data_alpha={\"mu\": alpha_dic[sigm2.mu], \"a\": alpha_dic[sigm2.a]}\n",
    "\n",
    "        \n",
    "    #json_data_alpha[\"E0_amp\"]=alpha_dic[E2.E0_maskable] #previous method\n",
    "    json_data_alpha[\"E0_amp\"]=alpha_dic[E2.E0_maskable_amp]\n",
    "\n",
    "    \n",
    "    json_data[\"Q10_distributed\"]=Q10_distributed\n",
    "    \n",
    "    \n",
    "    json_data[\"E0_distributed\"]=E0_distributed\n",
    "    \n",
    "    \n",
    "    json_data[\"I0_distributed\"]=I0_distributed\n",
    "    \n",
    "    if Q10_distributed:\n",
    "        json_data_alpha[\"Q10RBFweights\"]=alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]\n",
    "    else:\n",
    "        json_data_alpha[\"Q10\"]= alpha_dic_Q10[BW10_0TestFunc.BW_10] #cte bw\n",
    "\n",
    "\n",
    "\n",
    "    json_data_alpha[\"E0\"]=alpha_dic_E0[E2.E0_maskable]\n",
    "    \n",
    "    json_data[\"alpha\"]=json_data_alpha\n",
    "    \n",
    "    \n",
    "    with open(f'{results_folder}/optim_params_{CF}.json', 'w') as outfile:\n",
    "        json.dump(json_data, outfile, indent=4)\n",
    "\n",
    "    np.save(f'{results_folder}/err_list_{CF}.npy',\n",
    "            np.array(errs)/rms_2 )\n",
    "    \n",
    "    sig_rms=rms_2*1/ (vbw_maskingConds.n_conditions+ntch_maskingConds.n_conditions)*1/np.shape(vbw_signals_proc**2)[1]\n",
    "    sig_rms=np.sqrt(sig_rms)\n",
    "    np.savez(f'{results_folder}/err_list_{CF}.npz', sum_sq_err=np.array(errs), sum_sq_sig=rms_2, \n",
    "            noise_rms=noise_rms, sig_rms=sig_rms, snr=sig_rms/noise_rms)  #+info noise level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative for errors: computes errors on smaller interval (easier to compare to noise level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_bis=t0+alpha_tukey/2*(t1-t0)  #interval inside 100% for tukey window\n",
    "t1_bis=t1-alpha_tukey/2*(t1-t0)\n",
    "\n",
    "#Tukey window is applied to truncated signal -> needs to correct times\n",
    "t0_bis+=float(E2.t[0])\n",
    "t1_bis+=float(E2.t[0])\n",
    "\n",
    "\n",
    "print(f't0_bis: {t0_bis*1e3:.3f} ms, t1_bis: {t1_bis*1e3:.3f} ms')\n",
    "\n",
    "\n",
    "#ntch_maskingConds\n",
    "u1=u1_mat[0]    \n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model\n",
    "                        , suppression_model=suppAmount)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model\n",
    "                        , suppression_model=suppAmount)\n",
    "\n",
    "\n",
    "sq_err_ntch, sq_sig_ntch =get_sq_err_CAPs(E2, u1, ntch_signals_proc, t0_bis, t1_bis)\n",
    "\n",
    "\n",
    "#vbw_maskingConds\n",
    "\n",
    "\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, \n",
    "                         filter_model=filter_model, suppression_model=suppAmount)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, \n",
    "                         filter_model=filter_model, suppression_model=suppAmount)\n",
    "    \n",
    "    \n",
    "    \n",
    "sq_err_vbw, sq_sig_vbw=get_sq_err_CAPs(E2, u1, vbw_signals_proc, t0_bis, t1_bis)\n",
    "\n",
    "\n",
    "#vfreq_maskingConds\n",
    "if not import_weights:\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "\n",
    "    sq_err_vfreq, sq_sig_vfreq =get_sq_err_CAPs(E2, u1, vfreq_signals_proc, t0_bis, t1_bis)\n",
    "    sig_rms_vfreq=np.sqrt(np.sum(sq_sig_vfreq)/ vfreq_maskingConds.n_conditions )\n",
    "    err_rms_vfreq=np.sqrt(np.sum(sq_err_vfreq)/ vfreq_maskingConds.n_conditions )\n",
    "\n",
    "\n",
    "sig_rms2_list=[]\n",
    "errs2_rms_list=[]\n",
    "print('On 100% Tukey window: ')\n",
    "for (eps1, eps2, supp_text) in [(1, 0, '(various notch widths)'), \n",
    "                                (0, 1, '(various notch atten)'),\n",
    "                                (1,1, 'overall')]:\n",
    "    sig_rms2=(eps1*np.sum(sq_sig_vbw)+eps2*np.sum(sq_sig_ntch))*1/ (eps1*vbw_maskingConds.n_conditions+eps2*ntch_maskingConds.n_conditions)\n",
    "    sig_rms2=np.sqrt(sig_rms2)\n",
    "    sig_rms2_list.append(sig_rms2)\n",
    "    \n",
    "    errs2_rms=(eps1*np.sum(sq_err_vbw)+eps2*np.sum(sq_err_ntch))*1/ (eps1*vbw_maskingConds.n_conditions+eps2*ntch_maskingConds.n_conditions)\n",
    "    errs2_rms=np.sqrt(errs2_rms)\n",
    "    errs2_rms_list.append(errs2_rms)\n",
    "\n",
    "    print(f'  signal RMS {supp_text} : {sig_rms2*1e3:.3f}  μV, mean error (RMS): {errs2_rms*1e3:.3f}  μV (estimated noise level: {noise_rms*1e3:.3f}  μV)')\n",
    "\n",
    "if write_results:\n",
    "    if import_weights:\n",
    "        #HACK zero values\n",
    "        err_rms_vfreq=0\n",
    "        sig_rms_vfreq=0\n",
    "    np.savez(f'{results_folder}/err_list_{CF}_inside_window.npz',  \n",
    "            noise_rms=noise_rms, sig_rms=sig_rms2_list[2], sig_rms_ntch=sig_rms2_list[1],\n",
    "            sig_rms_vbw=sig_rms2_list[0],  err_rms=errs2_rms_list[2], err_rms_ntch=errs2_rms_list[1],\n",
    "            err_rms_vbw=errs2_rms_list[0],\n",
    "            sig_rms_vfreq=sig_rms_vfreq, err_rms_vfreq=err_rms_vfreq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #various notch atten\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    \n",
    "    #various notch widths\n",
    "    \n",
    "#     if io_func=='weibull':\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "#     else:\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "    pl.figure(figsize=(10,20))\n",
    "    axlist=plotExcitationPatterns(E2, plot_raw_excitation=True) # ylim_top=1\n",
    "    \n",
    "    for ax in axlist[0::2]:\n",
    "        ax.set_ylim(0,5)\n",
    "    pl.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "if not import_weights:\n",
    "    with torch.no_grad():\n",
    "        if io_func=='weibull':\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "        else:\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "\n",
    "        u1=u1_mat[0]\n",
    "        pl.figure(figsize=(12,20))\n",
    "        ax_list=plotSimulatedCAPs(E2, u1, max_plots=10, sig_exc=sig_exc_plot)\n",
    "        plotSimulatedCAPs(E2, CAParray=vfreq_signals_proc, axlist=ax_list, max_plots=10, plot_excitations=False, plotargs={\"color\":'C2'})\n",
    "\n",
    "\n",
    "        if save_figs:\n",
    "            pl.savefig(f'fitdata{CF}_vfreq_maskConds.svg')\n",
    "\n",
    "        pl.plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@interact_manual(I0=(-30, 0), scale=(10, 50), k=(0.5, 15), plot_only_learned=False)   #only works for weibull CDF\n",
    "def plot_v_attn_notch(I0, scale, k, plot_only_learned):\n",
    "    print('After learning: ')\n",
    "    print(wb_cdf2)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \n",
    "        if not(plot_only_learned):\n",
    "            wb_cdf_temp=WeibullCDF_IOFunc(constrained_at_Iref=True, Iref=wb_cdf2._Iref, I0=I0, \n",
    "                                      scale=scale, k=k)\n",
    "                \n",
    "            I=torch.linspace(-30, 30, 50)\n",
    "            pl.figure()\n",
    "\n",
    "            pl.plot(I, wb_cdf2(I))\n",
    "\n",
    "            pl.plot(I, wb_cdf_temp(I))\n",
    "            pl.xlim([-20, 30])\n",
    "            pl.title('Masking IO Function')\n",
    "            pl.xlabel('Power spectral density (dB)')\n",
    "            pl.show()\n",
    "        \n",
    "        if io_func=='weibull':\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "        else:\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "\n",
    "        u1=u1_mat[0]\n",
    "        pl.figure(figsize=(12,20))\n",
    "        ax_list=plotSimulatedCAPs(E2, u1, ylim=[-40, 40], max_plots=10, sig_exc=sig_exc_plot)\n",
    "\n",
    "        \n",
    "        if io_func=='weibull' and not plot_only_learned:\n",
    "        \n",
    "            E_temp=ExcitationPatterns.copyRaw(E2)\n",
    "            E_temp.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf_temp, \n",
    "                                     filter_model=filter_model, suppression_model=suppAmount)\n",
    "        \n",
    "            plotSimulatedCAPs(E_temp, u1, axlist=ax_list, max_plots=10, sig_exc=sig_exc_plot)\n",
    "\n",
    "        plotSimulatedCAPs(E2, CAParray=ntch_signals_proc, axlist=ax_list, max_plots=10, \n",
    "                          plot_excitations=False, plotargs={\"color\":'C2'})\n",
    "    if save_figs:\n",
    "        pl.savefig(f'fitdata{CF}_ntch_maskConds.svg')\n",
    "    pl.plot()\n",
    "    \n",
    "plot_v_attn_notch(0, 15, 5, True) #hack learned curve (random params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #various bws\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "\n",
    "\n",
    "    #various notch widths\n",
    "    \n",
    "#     if io_func=='weibull':\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "#     else:\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "    pl.figure(figsize=(10,20))\n",
    "    axlist=plotExcitationPatterns(E2, plot_raw_excitation=True, max_plots=6) # ylim_top=1\n",
    "    \n",
    "    for ax in axlist[0::2]:\n",
    "        ax.set_ylim(0,5)\n",
    "    pl.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "@interact_manual(bw=(500, 3000), plot_only_learned=False)   #only works for weibull CDF\n",
    "def plot_vbw(bw, plot_only_learned):\n",
    "    max_plots=16\n",
    "    \n",
    "    u1=u1_mat[0]\n",
    "    pl.figure(figsize=(10,20))\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ax_list=plotSimulatedCAPs(E2, u1, ylim=[-10, 10], sig_exc=sig_exc_plot, max_plots=max_plots)\n",
    "        plotSimulatedCAPs(E2, CAParray=vbw_signals_proc, \n",
    "                          axlist=ax_list, plot_excitations=False, plotargs={\"color\":'C2'}, max_plots=max_plots)\n",
    "\n",
    "    if not plot_only_learned:\n",
    "        \n",
    "        BW10_0TestFunc2=constant_BW10(bw, requires_grad=False)\n",
    "\n",
    "        \n",
    "        if io_func=='weibull':\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc2, vbw_maskingConds, wb_cdf2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "        else:\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc2, vbw_maskingConds, sigm2, \n",
    "                                 filter_model=filter_model, suppression_model=suppAmount)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            plotSimulatedCAPs(E2, u1, ylim=[-10, 10], sig_exc=sig_exc_plot, max_plots=max_plots, axlist=ax_list)\n",
    "    \n",
    "    \n",
    "    if save_figs:\n",
    "        pl.savefig(f'fitdata{CF}_vbw_maskConds.svg')    \n",
    "    pl.plot()\n",
    "    \n",
    "    \n",
    "#plot_vbw(1000, True) #hack learned curve (random params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1=u1_mat[0]\n",
    "bw_arr=np.linspace(400, 3000, num= ((4000-500)//50+1) )\n",
    "sigs_ref=vbw_signals_proc\n",
    "errs=[]\n",
    "for bw in bw_arr:\n",
    "\n",
    "    BW10_0TestFunc2=constant_BW10(bw, requires_grad=False)\n",
    "\n",
    "\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc2, vbw_maskingConds, wb_cdf2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc2, vbw_maskingConds, sigm2, \n",
    "                             filter_model=filter_model, suppression_model=suppAmount)\n",
    "    excs = E2.get_tensor() \n",
    "    maskingConditions = E2.maskingConditions\n",
    "    err=0\n",
    "    for i, exc in zip(range(maskingConditions.n_conditions), excs):\n",
    "        exc_np = exc.detach().numpy()\n",
    "        CAP=np.convolve(exc_np, u1, mode='full')\n",
    "        t=E.t.numpy()\n",
    "        CAP=CAP[0:len(E2.t)]\n",
    "        err+=np.mean( (CAP-sigs_ref[i])**2)\n",
    "    errs.append(err/maskingConditions.n_conditions*1e6)\n",
    "    \n",
    "pl.plot(bw_arr, np.sqrt(errs))\n",
    "pl.xlabel('BW10 model (Hz)')\n",
    "\n",
    "pl.ylabel('Mean error (μV)')\n",
    "\n",
    "if save_figs:\n",
    "    pl.savefig(f'fitdata{CF}_BW10_errs.svg')\n",
    "\n",
    "\n",
    "ind_min=np.argmin(errs)\n",
    "print(f'estimated bw10: {bw_arr[ind_min]:.0f} Hz')\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/Q10gridsearch_{CF}.npz', bw=bw_arr, errs=errs, bw10_est=bw_arr[ind_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
