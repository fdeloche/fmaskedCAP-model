{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "expe_name='4-23'  #'1-22'\n",
    "\n",
    "#CF=8000\n",
    "CF=6000\n",
    "\n",
    "\n",
    "mode_CAP='C+R' #'R' #'C+R'\n",
    "\n",
    "E0_distributed=False #if True, E0 will be estimated from the main node of a distributed scheme (external process)\n",
    "#load params from E0_params.json\n",
    "Q10_distributed=False #if True, Q10 will be computed and estimated from the main node of a distributed scheme\n",
    "#load params from RBF_params.json\n",
    "I0_distributed=False #I0 for weibull cdf\n",
    "\n",
    "results_folder0=f'./results/fit{expe_name}-distrib/'  #if I0_distributed, loads wb cdf params from other folder\n",
    "\n",
    "backend=dist.Backend('GLOO')\n",
    "n_workers=2\n",
    "rank=1\n",
    "\n",
    "filter_model='gaussian'\n",
    "\n",
    "load_json_optim_params=False #if True load optim params from optim_params.json\n",
    "\n",
    "write_results=False #write ur, I/O func, Q10, lat params in files\n",
    "#to run (distributed): papermill -p E0_distributed True -p Q10_distributed True -p n_workers 5 -p rank 1 -p CF 4000 Fit\\ data.ipynb fitdata4000.ipynb\n",
    "\n",
    "\n",
    "results_name=''  #if not blank, will save all the results in a folder with results_name (also loads param from this folder, like optim params)\n",
    "results_folder=None\n",
    "if results_name != '':\n",
    "    results_folder=f'./results/fit{expe_name}-{results_name}/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if expe_name == '4-23':\n",
    "    from fit_data_4_23_common import *\n",
    "    from fit_data_4_23_list_maskers import *\n",
    "elif expe_name=='1-22':\n",
    "    from fit_data_1_22_common import *\n",
    "    from fit_data_1_22_list_maskers import *\n",
    "\n",
    "pl.style.use('fivethirtyeight')\n",
    "\n",
    "#mpl.rc('figure', figsize=(10,8))\n",
    "mpl.rcParams['axes.facecolor']='white'  \n",
    "mpl.rcParams['figure.facecolor'] = '1'\n",
    "\n",
    "from optim import *\n",
    "\n",
    "from rbf import RBFNet\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "if results_folder is None:\n",
    "    if Q10_distributed:\n",
    "        if I0_distributed:\n",
    "\n",
    "            results_folder=f'./results/fit{expe_name}-distrib/I0_distrib/'\n",
    "        else:\n",
    "            results_folder=f'./results/fit{expe_name}-distrib/'\n",
    "\n",
    "    else:\n",
    "        results_folder=f'./results/fit{expe_name}/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common for all CFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see fit_data_1_22_common.py\n",
    "\n",
    "plot_main_CAPs()\n",
    "plot_CAP_with_window()\n",
    "\n",
    "\n",
    "\n",
    "if expe_name == '1-22':\n",
    "    pass\n",
    "    plot_CAP_w_wo_filter()\n",
    "\n",
    "#NB: the plots below depend on the choice of the signal used for the estimation of ur\n",
    "plot_raw_excitation_deconv()\n",
    "plot_figures_narrowband_analysis()\n",
    "plot_figures_narrowband_analysis_deconv()\n",
    "plot_estimated_latencies_deconv()\n",
    "plot_latencies_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify capData if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode_CAP !='C+R':\n",
    "    capData=CAPData(data_folder, listFiles, begin_ind=28, end_ind=1884, mode=mode_CAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first estimation I/O curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_results and not(os.path.exists(results_folder)):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap=[]\n",
    "rms=[]\n",
    "masker_list=ntch_masker_lists[CF]  #, 'broadband_noise' \n",
    "masker_list=[st.replace('-', '_').replace('.json', '') for st in masker_list]\n",
    "\n",
    "\n",
    "reg_exp=ntch_regexps[CF]\n",
    "\n",
    "for i, masker in enumerate(masker_list):\n",
    "    sig=capData.get_signal_by_name(masker)\n",
    "    if not(re.match(reg_exp, masker)):\n",
    "        continue\n",
    "    sig=process_signal(sig)\n",
    "    broadband_sig_trunc=process_signal(broadband2)\n",
    "    \n",
    "    #REF broadband\n",
    "    cap_amp=np.max(sig-broadband_sig_trunc)-np.min(sig-broadband_sig_trunc)\n",
    "    #HACK\n",
    "    if '17dB' in masker:\n",
    "        cap_amp*=-1\n",
    "    cap.append(cap_amp)\n",
    "    #rms.append(np.std(sig-broadband_sig_trunc))\n",
    "    \n",
    "    #cap.append(np.max(sig)-np.min(sig))\n",
    "    #rms.append(np.std(sig-broadband_sig_trunc))\n",
    "    pl.plot(t2, sig-broadband_sig_trunc, label=masker)\n",
    "\n",
    "\n",
    "pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "pl.show()\n",
    "\n",
    "pl.figure(figsize=(8, 6))\n",
    "\n",
    "if expe_name =='1-22':\n",
    "    attns=-np.array([55, 50, 45,40,35,32,29,26,23])   # 20])\n",
    "elif expe_name == '4-23':\n",
    "    attns=-attns_arrays[CF]\n",
    "pl.plot(attns, cap, '+', label='max-min')\n",
    "\n",
    "#pl.plot(-np.array([55,50,45,40,37,34,31,28,25,22]), np.array(rms)*10, label='rms x10')\n",
    "pl.legend()\n",
    "pl.xlabel('Notch attenuation')\n",
    "pl.ylabel('Amplitude difference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigm=SigmoidIOFunc(0, 0)\n",
    "#maskamount=1-(  (cap-np.amin(cap)) /np.amax(cap-np.amin(cap)) )\n",
    "maskamount=1-(cap/np.amax(cap)) \n",
    "\n",
    "I_pts=I0+attns\n",
    "#sigm.fit_data(I_pts, maskamount, set_mmax=True)\n",
    "\n",
    "#HACK enforce masking=100% at attn20\n",
    "#sigm.mmax.data*=1/sigm(I0-20)\n",
    "\n",
    "sigm.fit_data(I_pts, maskamount, constrained_at_Iref=True, Iref=I0-20)\n",
    "\n",
    "wb_cdf=WeibullCDF_IOFunc()\n",
    "\n",
    "wb_cdf.fit_data(I_pts, maskamount, constrained_at_Iref=True, Iref=I0-20)\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/maskamountCAP_{CF}.npz', I_pts=I_pts, maskamount=maskamount)\n",
    "    sigm.write_to_npz(f'{results_folder}/sigmIO_1st_estim_{CF}.npz')\n",
    "    wb_cdf.write_to_npz(f'{results_folder}/wbcfdIO_1st_estim_{CF}.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=np.linspace(-30, 25)\n",
    "\n",
    "\n",
    "pl.plot(I, sigm(torch.tensor(I)), label='fit sigm')\n",
    "pl.plot(I, wb_cdf(torch.tensor(I)), label='fit wbcdf')\n",
    "pl.xlabel('Power spectral density (dB)')\n",
    "\n",
    "pl.plot(I_pts, maskamount, '+', markersize=10, markeredgewidth=3)\n",
    "\n",
    "pl.plot(I0-20, 1, '+', markersize=10, markeredgewidth=3, color='purple')\n",
    "\n",
    "pl.ylabel('masking (max: broadband)')\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reg_exp=ntch_regexps[CF]  #previous method\n",
    "\n",
    "fln_list=ntch_masker_lists[CF]\n",
    "masker_list=[st.replace('-', '_').replace('.json', '') for st in fln_list]\n",
    "reg_exp=')|('.join(masker_list)\n",
    "reg_exp='('+reg_exp+')'\n",
    "\n",
    "ntch_maskerNames, ntch_maskingConds, ntch_signals =capData.get_batch_re(reg_exp)\n",
    "ntch_maskingConds.set_amp0_dB(I0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HACK pad maskers >12e3 to avoid issues with latencies (equivalent to taking the difference\n",
    "#  excitations of maskers - excitation 'broadband noise')\n",
    "ntch_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)\n",
    "ntch_maskingConds.pad_maskers2() #same thing for low freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_sigma=(1e-4)/(t2[1]-t2[0])\n",
    "ntch_signals_proc=process_signal2(ntch_signals, gauss_sigma=gauss_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "for maskerName, sig in zip(ntch_maskerNames, ntch_signals_proc):\n",
    "    pl.plot(t2, sig, label=maskerName)\n",
    "\n",
    "pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation ur\n",
    "\n",
    "sig=capData.get_signal_by_name('5_notch8000_bw2300_40dB')\n",
    "sig2=process_signal(sig)\n",
    "\n",
    "pl.plot((t2-3e-3)*1e3, sig2-broadband_proc)\n",
    "\n",
    "#pl.plot(t2*1e3, sig2bis-broadband_proc)\n",
    "\n",
    "t_shift=6e-3-3e-3 #excitation coincides with CM\n",
    "\n",
    "ur0=sig2-broadband_proc\n",
    "ur0=np.roll(ur0,  -int(t_shift*48828) )\n",
    "pl.plot((t2-3e-3)*1e3, ur0)\n",
    "#pl.xlim([4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latencies model\n",
    "\n",
    "#lat_model=lat_above4k\n",
    "#lat_model=lat\n",
    "\n",
    "#lat_model=PowerLawLatencies.shift(lat_model, 6e-3-1e-3)   #t0: start CM-1ms\n",
    "\n",
    "#HACK as latencies are very small (sampling issues), manual dilatation\n",
    "\n",
    "#lat_model=lat_above4k\n",
    "lat_model=lat\n",
    "lat_shifted=PowerLawLatencies.shift(lat_model, 6e-3-1e-3)   #t0: start CM-1ms\n",
    "lat_shifted.name='true latencies'\n",
    "\n",
    "use_bincount=True\n",
    "if use_bincount:\n",
    "    lat_model=lat_shifted\n",
    "else:\n",
    "    #HACK as latencies are very small (sampling issues), manual dilatation\n",
    "    lat_model=PowerLawLatencies.fromPts(0.0056, 10000, 0.007, 800, name= 'dilatated (hack)')\n",
    "    #not required with bincount\n",
    "    #lat_model=PowerLawLatencies.fromPts(0.0057, 9500, 0.0062, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test single lat model\n",
    "singleLat=False\n",
    "if singleLat or use_bincount:\n",
    "    if CF>6500:\n",
    "        f_min=4000\n",
    "        f_max=12000\n",
    "    elif CF < 5500:\n",
    "        if CF<4500:\n",
    "            f_min=600\n",
    "            f_max=7500\n",
    "        else:\n",
    "            f_min=2200\n",
    "            f_max=8000\n",
    "    else:\n",
    "        f_min=2500\n",
    "        f_max=9000\n",
    "        \n",
    "    if E0_distributed:\n",
    "        with open('E0_params.json') as f:\n",
    "            params = json.load(f)\n",
    "            f_min=float(params['f_min'])\n",
    "            f_max=float(params['f_max'])\n",
    "        \n",
    "    if singleLat:\n",
    "        lat_model = SingleLatency(6e-3, f_min=f_min, f_max=f_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if singleLat or use_bincount:\n",
    "    \n",
    "    if E0_distributed:\n",
    "        with open('E0_params.json') as f:\n",
    "            params = json.load(f)\n",
    "            m=int(params['m'])\n",
    "    else:\n",
    "        m=400\n",
    "    E0=1/2*np.ones((m,))\n",
    "    \n",
    "    pl.plot(np.linspace(f_min*1e-3, f_max*1e-3, m), E0)\n",
    "    pl.xlabel('Frequency (kHz)')\n",
    "    pl.ylabel('Init raw excitation')\n",
    "else:\n",
    "    m=72\n",
    "    E0_temp=sg.windows.tukey(m, alpha=0.5) \n",
    "    E0=np.zeros_like(t2)\n",
    "    ind_begin=int((t_shift-1e-3)*48828)\n",
    "    ind_end=int((t_shift-1e-3)*48828)+m\n",
    "    E0[ind_begin:ind_end]=E0_temp\n",
    "\n",
    "    pl.plot(t2*1e3, E0)\n",
    "    pl.title('Init raw excitation')\n",
    "    pl.xlabel('t (ms)')\n",
    "    pl.ylabel('Amp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLatencies(lat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=ExcitationPatterns(t2, E0, use_bincount=use_bincount, bincount_fmin=f_min, bincount_fmax=f_max)  #no non-maskable part\n",
    "#E=ExcitationPatterns(t2, E_temp.E0_maskable)\n",
    "\n",
    "#NB: first model for estimation of ur, cte bandwith, fixed\n",
    "if CF>4000:\n",
    "    BW10_0=2000\n",
    "else:\n",
    "    BW10_0=1300\n",
    "BW10_0Func=constant_BW10(BW10_0, requires_grad=False) \n",
    "\n",
    "#E.set_masking_model(lat_model, BW10_0Func, ntch_maskingConds, sigm, filter_model=filter_model)\n",
    "E.set_masking_model(lat_model, BW10_0Func, ntch_maskingConds, wb_cdf, filter_model=filter_model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pl.figure(figsize=(10,20))\n",
    "plotMaskingAmountExcitations(BW10_0Func, ntch_maskingConds, sigm)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pl.figure(figsize=(10,20))\n",
    "plotExcitationPatterns(E, plot_raw_excitation=True) # ylim_top=1\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "maskAmounts, excs = E.get_tensors() \n",
    "\n",
    "for i in range(5):\n",
    "    pl.plot(E.t*1e3, maskAmounts[i], label=ntch_maskerNames[i])\n",
    "pl.xlim([3,6])\n",
    "pl.ylim([0,1])\n",
    "pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimation ur\n",
    "\n",
    "maskAmounts, excs = E.get_tensors() \n",
    "\n",
    "nb_steps=20\n",
    "alpha=np.linspace(0.5, 0.05, nb_steps)\n",
    "\n",
    "EPs_fft=np.fft.rfft(excs, axis=1)\n",
    "CAPs_fft=np.fft.rfft(ntch_signals_proc, axis=1)\n",
    "#u1_mat=np.tile(ur0, (ntch_maskingConds.n_conditions, 1))\n",
    "u1_mat=np.zeros_like(ntch_signals_proc)\n",
    "filter_mat  = (t2>7.5e-3)+(t2<3.2e-3)\n",
    "filter_mat=np.tile(filter_mat, (ntch_maskingConds.n_conditions, 1))\n",
    "#filter_mat=np.zeros_like(ntch_signals_proc, dtype=bool)\n",
    "#proj_fft=E.get_projector_fft()\n",
    "\n",
    "weights=np.sqrt(np.sum(excs.clone().detach().numpy()**2, axis=1))\n",
    "for i in range(1, nb_steps+1):\n",
    "    du=deconv_newton_step(u1_mat, EPs_fft, CAPs_fft, eps_ridge=0)   #TODO proj_fft\n",
    "    #du=deconv_grad(u1_mat, EPs_fft, CAPs_fft)\n",
    "    \n",
    "    u1_mat-=alpha[i-1]*du\n",
    "    #proj 1 \n",
    "    u1_mat[filter_mat]=np.zeros_like(u1_mat[filter_mat])\n",
    "    #proj 2\n",
    "\n",
    "    #u1_mat_mean=np.mean(u1_mat, axis=0)[None, :]\n",
    "    \n",
    "    \n",
    "    #HACK waiting for proj_fft\n",
    "    u1_mat_mean=np.average(u1_mat, axis=0, weights=weights)[None, :]\n",
    "    u1_mat=np.repeat(u1_mat_mean, ntch_maskingConds.n_conditions, axis=0)\n",
    "     \n",
    "    '''\n",
    "    for i in range(5):\n",
    "            pl.figure()\n",
    "            name=ntch_maskerNames[i]\n",
    "            pl.plot(u1_mat[i], label=name, color=f'C{i}')\n",
    "            #pl.plot( np.abs(EPs_fft[i]))\n",
    "            pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    '''\n",
    "    \n",
    "\n",
    "    if i==nb_steps:\n",
    "        pl.figure()\n",
    "        pl.title(f'Step {i} (deconv + proj)')\n",
    "        #pl.plot(t, u0, label='u0 (truth)')\n",
    "        pl.plot(t2, u1_mat[0], label='u0 (estimated)')\n",
    "        pl.legend()\n",
    "        #pl.savefig('ur_8kHz_Q395.svg')\n",
    "        pl.show()\n",
    "        \n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/ur_{CF}.npz', t2=t2, ur=u1_mat[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maskers and signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verious freqs. (for estimating E0)\n",
    "fln_list=vfreq_fln_lists[CF]\n",
    "masker_list=[st.replace('-', '_').replace('.json', '') for st in fln_list]\n",
    "\n",
    "reg_exp=')|('.join(masker_list)\n",
    "reg_exp='('+reg_exp+')'\n",
    "vfreq_maskerNames, vfreq_maskingConds, vfreq_signals =capData.get_batch_re(reg_exp)\n",
    "vfreq_signals_proc=process_signal2(vfreq_signals, gauss_sigma=gauss_sigma)\n",
    "vfreq_maskingConds.set_amp0_dB(I0)\n",
    "vfreq_maskingConds.pad_maskers(f_thr=11000, f_max=np.Inf)\n",
    "\n",
    "#various bws (for estimating Q10)\n",
    "fln_list=vbw_fln_lists[CF]\n",
    "masker_list=[st.replace('-', '_').replace('.json', '') for st in fln_list]\n",
    "reg_exp=')|('.join(masker_list)\n",
    "reg_exp='('+reg_exp+')'\n",
    "vbw_maskerNames, vbw_maskingConds, vbw_signals =capData.get_batch_re(reg_exp)\n",
    "vbw_signals_proc=process_signal2(vbw_signals, gauss_sigma=gauss_sigma)\n",
    "vbw_maskingConds.set_amp0_dB(I0)\n",
    "#HACK\n",
    "vbw_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "signals_proc=vfreq_signals_proc\n",
    "maskingConds=vfreq_maskingConds\n",
    "\n",
    "E_temp=ExcitationPatterns.copyRaw(E, requires_grad=True)\n",
    "\n",
    "#E_temp.set_masking_model(lat_model, BW10_0Func, maskingConds, sigm, filter_model=filter_model)\n",
    "E_temp.set_masking_model(lat_model, BW10_0Func, maskingConds, wb_cdf, filter_model=filter_model)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "nb_steps=30\n",
    "alpha=20\n",
    "n_dim=7 #projection of gradient on n_dim first dimensions (Fourier basis)\n",
    "\n",
    "alpha_dic={E_temp.E0_maskable: alpha}\n",
    "optim_steps(E_temp, u1_mat[0], signals_proc, alpha_dic, \n",
    "            nb_steps=nb_steps, n_dim_E0=n_dim, plot_E0_graph=True, step_plots=5 )\n",
    "pl.show()\n",
    "E.E0_maskable.data=E_temp.E0_maskable.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tuning (I/O curve, Q10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try more accurate estimation of i/o curve\n",
    "\n",
    "signals_proc=ntch_signals_proc\n",
    "maskingConds=ntch_maskingConds\n",
    "\n",
    "\n",
    "io_func = 'weibull' \n",
    "#io_func= 'sigm'\n",
    "\n",
    "#sigm2=SigmoidIOFunc(sigm.mu.numpy(), sigm.a.numpy(), Iref=I0-20, constrained_at_Iref=True, requires_grad=True)\n",
    "\n",
    "\n",
    "#sigm2=SigmoidIOFunc(sigm.mu.numpy(), sigm.a.numpy(), Iref=I0-20, constrained_at_Iref=True, requires_grad=True)\n",
    "\n",
    "sigm2=SigmoidIOFunc(5., 0.25, Iref=I0-20, constrained_at_Iref=True, requires_grad=True)\n",
    "\n",
    "\n",
    "# wb_cdf2=WeibullCDF_IOFunc(I0=wb_cdf.I0.numpy(),\n",
    "#     scale=wb_cdf.scale.numpy(),\n",
    "#     mmax=wb_cdf.mmax.numpy(),\n",
    "#     k=wb_cdf.k.numpy(),\n",
    "#     requires_grad=True,\n",
    "#     constrained_at_Iref=True,\n",
    "#     Iref=I0-20)\n",
    "\n",
    "# wb_cdf2=WeibullCDF_IOFunc(I0=-21.,\n",
    "#     scale=30.,\n",
    "#     k=13.,\n",
    "#     mmax=1.,\n",
    "#     requires_grad=True,\n",
    "#     constrained_at_Iref=True,\n",
    "#     Iref=I0-20)\n",
    "\n",
    "if expe_name=='4-23':\n",
    "    k_cdf=3.\n",
    "    I0_cdf=-24.\n",
    "else:\n",
    "    I0_cdf=-20.\n",
    "    k_cdf=5.\n",
    "    \n",
    "wb_cdf2=WeibullCDF_IOFunc(I0=I0_cdf,\n",
    "    scale=30.,\n",
    "    k=k_cdf,\n",
    "    mmax=1.,\n",
    "    requires_grad=True,\n",
    "    constrained_at_Iref=True,\n",
    "    Iref=I0-20)\n",
    "\n",
    "#E2=ExcitationPatterns(t2, E0, requires_grad=True)  #no non-maskable part\n",
    "E2=ExcitationPatterns.copyRaw(E, requires_grad=True)\n",
    "if Q10_distributed or E0_distributed:   \n",
    "    #init group\n",
    "    if not(dist.is_initialized()):\n",
    "        dist.init_process_group(backend, init_method='tcp://127.0.0.1:1234', world_size=n_workers, rank=rank, \n",
    "                                timeout=datetime.timedelta(0, 20))  \n",
    "    \n",
    "    if Q10_distributed:\n",
    "        Q10rbf=Q10RBFNet.create_from_jsonfile('RBF_params.json')\n",
    "        #update weights (have to be sent by main process)\n",
    "        Q10rbf.update_weights()\n",
    "        BW10_0TestFunc=Q10RBFNet_BW10(Q10rbf)\n",
    "    #Not needed to load weights for E0 as should be initialized at 1 anyway\n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    BW10_0TestFunc=constant_BW10(2000., requires_grad=True)\n",
    "    \n",
    "    \n",
    "if I0_distributed:\n",
    "    I0_rbf=RBFNet.create_from_jsonfile('RBF_I0_params.json')\n",
    "    wb_cdf2.set_I0_w_RBFNet(I0_rbf)\n",
    "    #update weights (have to be sent by main process)\n",
    "    I0_rbf.update_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim params\n",
    "#NOTE: params not imported from json file for first optim(?)\n",
    "\n",
    "alpha=30\n",
    "alpha_Q10=3e7\n",
    "\n",
    "#for estimation of E0\n",
    "n_dim=7 #projection of gradient on n_dim first harmomics (Fourier basis)\n",
    "\n",
    "\n",
    "if io_func=='weibull':\n",
    "    #alpha_dic={wb_cdf2.I0: 30*alpha, wb_cdf2.scale: 5*alpha, wb_cdf2.k: 30*alpha}\n",
    "    alpha_dic={wb_cdf2.scale: alpha, wb_cdf2.k: 10*alpha}\n",
    "    \n",
    "    if I0_distributed:\n",
    "        alpha_dic[wb_cdf2.rbfNet.l2.weight]=0.005*alpha\n",
    "    else:\n",
    "        alpha_dic[wb_cdf2.I0]=10*alpha\n",
    "        \n",
    "    #alpha_dic={wb_cdf2.I0: 0.1*alpha, wb_cdf2.scale: 0.05*alpha, wb_cdf2.k: 0.6*alpha}\n",
    "else:\n",
    "    alpha_dic={sigm2.mu: 0.01*alpha, sigm2.a: 0.005*alpha}\n",
    "\n",
    "#alpha_dic[BW10_0TestFunc.BW_10]=alpha\n",
    "#alpha_dic[E2.E0_maskable]=0.1*alpha  #/!| with sum_grad_E0 set to True #previous method to modify E0 amp\n",
    "alpha_dic[E2.E0_maskable_amp]=0.1*alpha  \n",
    "\n",
    "\n",
    "alpha_dic_Q10={}\n",
    "\n",
    "if Q10_distributed:\n",
    "    alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]=0.05*alpha\n",
    "else:\n",
    "    alpha_dic_Q10[BW10_0TestFunc.BW_10]=alpha_Q10 #cte bw\n",
    "    \n",
    "\n",
    "\n",
    "alpha_dic_E0={E2.E0_maskable: 0.2*alpha}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first optim I/O func (if not I0 distributed)\n",
    "\n",
    "nb_stepsIO=50\n",
    "\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "if not(I0_distributed):\n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "                nb_steps=nb_stepsIO, #sum_grad_E0=True, \n",
    "                                plot_masking_I0_graph=True,\n",
    "               step_plots=5)\n",
    "else:\n",
    "    #import params (k, scale)\n",
    "    wb_cdf_temp=WeibullCDF_IOFunc.load_from_npz(f'{results_folder0}/wbcfdIO_{CF}.npz')\n",
    "    wb_cdf2.k.data=wb_cdf_temp.k.data\n",
    "    wb_cdf2.scale.data=wb_cdf_temp.scale.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params for main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note change params for I/O func\n",
    "#TODO write in json?\n",
    "#loads params from json file\n",
    "if io_func=='weibull':\n",
    "    if not(I0_distributed):\n",
    "        alpha_dic[wb_cdf2.I0]=0.5*alpha\n",
    "    alpha_dic[wb_cdf2.scale]= 0.05*alpha\n",
    "    alpha_dic[wb_cdf2.k]= 0.5*alpha    \n",
    "\n",
    "n_it=100  #100\n",
    "nb_steps=3 #5\n",
    "\n",
    "\n",
    "if load_json_optim_params:\n",
    "    with open('optim_params.json') as f:\n",
    "        dic_params=json.load(f)\n",
    "    \n",
    "    n_it=dic_params['n_it']\n",
    "    \n",
    "    nb_steps=dic_params['nb_steps']\n",
    "    n_dim=dic_params['n_dim']\n",
    "    step_values=dic_params['alpha']\n",
    "    if io_func=='weibull':\n",
    "        if I0_distributed:\n",
    "            alpha_dic[wb_cdf2.rbfNet.l2.weight]=float(step_values['I0RBFweights'])\n",
    "\n",
    "\n",
    "        else:\n",
    "            alpha_dic[wb_cdf2.I0]=float(step_values['I0'])\n",
    "\n",
    "\n",
    "        alpha_dic[wb_cdf2.scale]= float(step_values['scale'])\n",
    "        alpha_dic[wb_cdf2.k]= float(step_values['k'] )\n",
    "    else:\n",
    "        \n",
    "        alpha_dic[sigm2.mu]= float(step_values['sigm_mu'])\n",
    "        alpha_dic[sigm2.a]= float(step_values['sigm_a']\n",
    "    alpha_dic[E2.E0_maskable_amp]=float(step_values['E0_amp'])\n",
    "        \n",
    "\n",
    "    if Q10_distributed:\n",
    "        alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]=float(step_values['Q10RBFweights'])\n",
    "    else:\n",
    "        alpha_dic_Q10[BW10_0TestFunc.BW_10]=float(step_values['Q10']) #cte bw\n",
    "                                  \n",
    "    \n",
    "    alpha_dic_E0[E2.E0_maskable]=float(step_values['E0'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optim (main loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tot_steps=3*n_it*nb_steps\n",
    "\n",
    "errs=[]\n",
    "errs_total=[]\n",
    "\n",
    "pl.figure(figsize=(6, 12))\n",
    "\n",
    "for i in range(n_it):\n",
    "    \n",
    "    if Q10_distributed or E0_distributed or I0_distributed: #informs the main node that optim is still in process\n",
    "        optim_done_hand=dist.isend(torch.tensor(nb_steps, dtype=torch.int32),0, tag=16)\n",
    "        optim_done_hand.wait()\n",
    "    \n",
    "    if E0_distributed:  #update E0\n",
    "        hand = dist.irecv(E2.E0_maskable, src=0, tag=8)\n",
    "        hand.wait()\n",
    "    if Q10_distributed:\n",
    "        Q10rbf.update_weights()\n",
    "    \n",
    "    if I0_distributed:\n",
    "        I0_rbf.update_weights()\n",
    "       \n",
    "    #E0\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "    if i==0:\n",
    "        axes=None\n",
    "        ind_plots=None\n",
    "        \n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], vfreq_signals_proc, alpha_dic_E0, \n",
    "        nb_steps=nb_steps, \n",
    "        n_dim_E0=n_dim, \n",
    "        E0_distributed=E0_distributed,                        \n",
    "         #E0_t_min=t_min_E0, E0_t_max=t_max_E0, k_mode_E0=k_mode_E0,\n",
    "        plot_E0_graph=True, plot_masking_I0_graph=True,\n",
    "        plot_Q10=True, fc_ref_Q10=CF, step_plots=5, axes=axes, ind_plots=ind_plots, \n",
    "        step0=(3*i)*nb_steps, tot_steps=tot_steps) \n",
    "        \n",
    "    err0=err_list[-1]\n",
    "\n",
    "    #I/O Func (+ amp E0)\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "                nb_steps=nb_steps, #sum_grad_E0=True, \n",
    "                plot_E0_graph=True, plot_masking_I0_graph=True,\n",
    "                plot_Q10=True, fc_ref_Q10=CF,\n",
    "               step_plots=5, axes=axes, ind_plots=ind_plots, step0=(3*i+1)*nb_steps,\n",
    "                 tot_steps=tot_steps, I0_distributed=I0_distributed)\n",
    "    err1=err_list[-1]\n",
    "\n",
    "#     #Q10\n",
    "    \n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm, filter_model=filter_model)\n",
    "\n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], vbw_signals_proc, alpha_dic_Q10, \n",
    "            nb_steps=nb_steps, sum_grad_E0=True, \n",
    "            plot_E0_graph=True, plot_masking_I0_graph=True,\n",
    "            plot_Q10=True, fc_ref_Q10=CF,\n",
    "           step_plots=5, axes=axes, ind_plots=ind_plots, step0=(3*i+2)*nb_steps,\n",
    "               tot_steps=tot_steps, #verbose=i%5,\n",
    "               Q10_distributed=Q10_distributed)\n",
    "    err2=err_list[-1]\n",
    "    errs.append( (err1+err2))  #errors are summed only on notched noise maskers (update I/O curve and Q10)\n",
    "    #nb: possible duplicates\n",
    "    errs_total.append( (err0+err1+err2))\n",
    "\n",
    "    \n",
    "if Q10_distributed or E0_distributed or I0_distributed: #informs the main node that optim is done\n",
    "    optim_done_hand=dist.isend(torch.tensor(0, dtype=torch.int32),0, tag=16)\n",
    "    #optim_done_hand.wait()\n",
    "        \n",
    "pl.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_2=np.sum(vbw_signals_proc**2)+np.sum(ntch_signals_proc**2)  \n",
    "rms_2b=rms_2+np.sum(vfreq_signals_proc**2)  \n",
    "\n",
    "pl.figure()\n",
    "pl.plot(np.arange(len(errs)), errs/rms_2*100, label='notched noise maskers')\n",
    "\n",
    "#pl.plot(np.arange(len(errs)), errs_total/rms_2*100, label='all maskers (w/ duplicates)')\n",
    "\n",
    "pl.xlabel('Iterations')\n",
    "pl.ylabel('Error (% variance)')\n",
    "\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data\n",
    "if write_results:\n",
    "\n",
    "    \n",
    "    \n",
    "    if io_func=='weibull':\n",
    "        wb_cdf2.write_to_npz(f'{results_folder}/wbcfdIO_{CF}.npz')\n",
    "    else:\n",
    "        sigm2.write_to_npz(f'{results_folder}/sigmIO_{CF}.npz')    \n",
    "    \n",
    "    if isinstance(lat_model, SingleLatency):\n",
    "        np.savez(f'{results_folder}/E0_{CF}.npz', f=lat_model.get_f_linspace(len(t2)).detach().numpy(),\n",
    "                E0=E2.E0_maskable.detach().numpy(), lat=lat_model.t0, E0_amp=E2.E0_maskable_amp.detach().numpy())\n",
    "    else:\n",
    "        if use_bincount: \n",
    "            np.savez(f'{results_folder}/E0_{CF}.npz', f=E2.bincount_f.detach().numpy(),\n",
    "                E0=E2.E0_maskable.detach().numpy(), E0_amp=E2.E0_maskable_amp.detach().numpy())\n",
    "            \n",
    "       \n",
    "        #save lat model\n",
    "        lat_model.write_to_npz(f'{results_folder}/lat_{CF}.npz') #Note: normally lat does not depend on CF but it could\n",
    "        \n",
    "    if Q10_distributed:\n",
    "        pass\n",
    "\n",
    "    Q10optim= CF/E2.bw10Func(torch.tensor(CF, dtype=torch.float32))\n",
    "    np.save(f'{results_folder}/Q10optim_{CF}.npy',\n",
    "            Q10optim.detach().numpy() )\n",
    "    \n",
    "    \n",
    "    #write params\n",
    "    \n",
    "    json_data={}\n",
    "    json_data[\"n_it\"]=n_it\n",
    "    json_data[\"nb_steps\"]=nb_steps\n",
    "    json_data[\"tot_steps\"]=tot_steps\n",
    "\n",
    "    alpha=30\n",
    "    alpha_Q10=3e7\n",
    "\n",
    "\n",
    "    #for estimation of E0\n",
    "    json_data[\"n_dim\"]=n_dim \n",
    "\n",
    "    if io_func=='weibull':\n",
    "        json_data_alpha={\"scale\": alpha_dic[wb_cdf2.scale],\n",
    "                         \"k\": alpha_dic[wb_cdf2.k]}\n",
    "        if I0_distributed:\n",
    "            json_data_alpha[\"I0_rbf_weights\"]=alpha_dic[wb_cdf2.rbfNet.l2.weight]\n",
    "        else:\n",
    "            json_data_alpha[\"I0\"]=alpha_dic[wb_cdf2.I0]\n",
    "    else:\n",
    "        json_data_alpha={\"mu\": alpha_dic[sigm2.mu], \"a\": alpha_dic[sigm2.a]}\n",
    "\n",
    "        \n",
    "    #json_data_alpha[\"E0_amp\"]=alpha_dic[E2.E0_maskable] #previous method\n",
    "    json_data_alpha[\"E0_amp\"]=alpha_dic[E2.E0_maskable_amp]\n",
    "\n",
    "    \n",
    "    json_data[\"Q10_distributed\"]=Q10_distributed\n",
    "    \n",
    "    \n",
    "    json_data[\"E0_distributed\"]=E0_distributed\n",
    "    \n",
    "    \n",
    "    json_data[\"I0_distributed\"]=I0_distributed\n",
    "    \n",
    "    if Q10_distributed:\n",
    "        json_data_alpha[\"Q10RBFweights\"]=alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]\n",
    "    else:\n",
    "        json_data_alpha[\"Q10\"]= alpha_dic_Q10[BW10_0TestFunc.BW_10] #cte bw\n",
    "\n",
    "\n",
    "\n",
    "    json_data_alpha[\"E0\"]=alpha_dic_E0[E2.E0_maskable]\n",
    "    \n",
    "    json_data[\"alpha\"]=json_data_alpha\n",
    "    \n",
    "    \n",
    "    with open(f'{results_folder}/optim_params_{CF}.json', 'w') as outfile:\n",
    "        json.dump(json_data, outfile, indent=4)\n",
    "\n",
    "    np.save(f'{results_folder}/err_list_{CF}.npy',\n",
    "            np.array(errs)/rms_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #various notch atten\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    #various notch widths\n",
    "    \n",
    "#     if io_func=='weibull':\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "#     else:\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "    pl.figure(figsize=(10,20))\n",
    "    plotExcitationPatterns(E2, plot_raw_excitation=True) # ylim_top=1\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "with torch.no_grad():\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    u1=u1_mat[0]\n",
    "    pl.figure(figsize=(12,20))\n",
    "    ax_list=plotSimulatedCAPs(E2, u1, ylim=[-0.03, 0.03], max_plots=10)\n",
    "    plotSimulatedCAPs(E2, CAParray=vfreq_signals_proc, axlist=ax_list, max_plots=10)\n",
    "    pl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@interact_manual(I0=(-30, 0), scale=(10, 50), k=(0.5, 15), plot_only_learned=True)   #only works for weibull CDF\n",
    "def plot_v_attn_notch(I0, scale, k, plot_only_learned):\n",
    "    print('After learning: ')\n",
    "    print(wb_cdf2)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \n",
    "        if not(plot_only_learned):\n",
    "            wb_cdf_temp=WeibullCDF_IOFunc(constrained_at_Iref=True, Iref=wb_cdf2._Iref, I0=I0, \n",
    "                                      scale=scale, k=k)\n",
    "                \n",
    "            I=torch.linspace(-30, 30, 50)\n",
    "            pl.figure()\n",
    "\n",
    "            pl.plot(I, wb_cdf2(I))\n",
    "\n",
    "            pl.plot(I, wb_cdf_temp(I))\n",
    "            pl.xlim([-20, 30])\n",
    "            pl.title('Masking IO Function')\n",
    "            pl.xlabel('Power spectral density (dB)')\n",
    "            pl.show()\n",
    "        \n",
    "        if io_func=='weibull':\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "        else:\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "        u1=u1_mat[0]\n",
    "        pl.figure(figsize=(12,20))\n",
    "        ax_list=plotSimulatedCAPs(E2, u1, ylim=[-0.03, 0.03], max_plots=10)\n",
    "\n",
    "        \n",
    "        if io_func=='weibull' and not plot_only_learned:\n",
    "        \n",
    "            E_temp=ExcitationPatterns.copyRaw(E2)\n",
    "            E_temp.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf_temp, filter_model=filter_model)\n",
    "        \n",
    "            plotSimulatedCAPs(E_temp, u1, axlist=ax_list, max_plots=10)\n",
    "\n",
    "        plotSimulatedCAPs(E2, CAParray=ntch_signals_proc, axlist=ax_list, max_plots=10)\n",
    "    pl.plot()\n",
    "    \n",
    "plot_v_attn_notch(0, 15, 5, True) #hack learned curve (random params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "u1=u1_mat[0]\n",
    "pl.figure(figsize=(10,14))\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    \n",
    "with torch.no_grad():\n",
    "    ax_list=plotSimulatedCAPs(E2, u1, ylim=[-0.01, 0.01])\n",
    "    plotSimulatedCAPs(E2, CAParray=vbw_signals_proc, axlist=ax_list)\n",
    "pl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_arr=np.linspace(500, 5000, num= ((4000-500)//50+1) )\n",
    "sigs_ref=vbw_signals_proc\n",
    "errs=[]\n",
    "for bw in bw_arr:\n",
    "\n",
    "    BW10_0TestFunc=constant_BW10(bw, requires_grad=False)\n",
    "\n",
    "\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "    excs = E2.get_tensor() \n",
    "    maskingConditions = E2.maskingConditions\n",
    "    err=0\n",
    "    for i, exc in zip(range(maskingConditions.n_conditions), excs):\n",
    "        exc_np = exc.detach().numpy()\n",
    "        CAP=np.convolve(exc_np, u1, mode='full')\n",
    "        t=E.t.numpy()\n",
    "        CAP=CAP[0:len(E2.t)]\n",
    "        err+=np.sum( (CAP-sigs_ref[i])**2)\n",
    "    errs.append(err)\n",
    "    \n",
    "pl.plot(bw_arr, errs)\n",
    "pl.xlabel('BW10 model (Hz)')\n",
    "\n",
    "pl.ylabel('square err')\n",
    "\n",
    "\n",
    "ind_min=np.argmin(errs)\n",
    "print(f'estimated bw10: {bw_arr[ind_min]:.0f} Hz')\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/Q10gridsearch_{CF}.npz', bw=bw_arr, errs=errs, bw10_est=bw_arr[ind_min])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#with gradient descent\n",
    "\n",
    "\n",
    "BW10_0TestFunc=constant_BW10(2000., requires_grad=True)\n",
    "\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm, filter_model=filter_model)\n",
    "\n",
    "nb_steps=30\n",
    "alpha=1e8\n",
    "\n",
    "alpha_dic[BW10_0TestFunc.BW_10]=alpha\n",
    "#alpha_dic[E2.E0_maskable]=alpha  #/!| with sum_grad_E0 set to True\n",
    "    \n",
    "\n",
    "optim_steps(E2, u1_mat[0], vbw_signals_proc, alpha_dic, \n",
    "            nb_steps=nb_steps, #sum_grad_E0=True, \n",
    "            #plot_E0_graph=True, plot_masking_I0_graph=True,\n",
    "            plot_Q10=True, fc_ref_Q10=CF,\n",
    "           step_plots=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
