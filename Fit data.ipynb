{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "expe_name='12-13' #2021: #'1-22' #'4-23' #'9-10' #'12-10' (Q333, partial data) #12-13 (Q336)  \n",
    "\n",
    "CF=5000  #1.5, 2.2, 3, 4, 5, 6, 8 kHz\n",
    "\n",
    "mode_CAP='C+R' #'R' #'C+R'\n",
    "\n",
    "E0_distributed=False #if True, E0 will be estimated from the main node of a distributed scheme (external process)\n",
    "#load params from E0_params.json\n",
    "Q10_distributed=False #if True, Q10 will be computed and estimated from the main node of a distributed scheme\n",
    "#load params from RBF_params.json\n",
    "I0_distributed=False #I0 for weibull cdf\n",
    "plus_lambda=False #if I0_distributed is True and plus_lambda is true, the output for the RBF network for I0 corresponds to I0 + lambda (scale)\n",
    "load_wbcdf=False\n",
    "\n",
    "#if I0_distributed or load_wbcdf True , loads wb cdf params from results_folder0\n",
    "results_folder0=f'./results/fit{expe_name}-distrib/'  \n",
    "\n",
    "backend=dist.Backend('GLOO')\n",
    "n_workers=2\n",
    "rank=1\n",
    "\n",
    "filter_model='gammatone_4'  #'gaussian'\n",
    "\n",
    "load_json_optim_params=True #if True load optim params from optim_params.json\n",
    "load_json_init_params=True #if True, will load ./init_params/{expe_name}/{CF}_init_params.json if exists\n",
    "\n",
    "write_results=False #write ur, I/O func, Q10, lat params in files\n",
    "#to run (distributed): papermill -p E0_distributed True -p Q10_distributed True -p n_workers 5 -p rank 1 -p CF 4000 Fit\\ data.ipynb fitdata4000.ipynb\n",
    "\n",
    "sig_exc_plot=0.  #1.2 #gauss sigma for excitation patterns in time (in number of bins). for plots only  #0 if no filtering\n",
    "save_figs=False\n",
    "\n",
    "results_name=''  #if not blank, will save all the results in a folder with results_name (also loads param from this folder, like optim params)\n",
    "results_folder=None\n",
    "if results_name != '':\n",
    "    results_folder=f'./results/fit{expe_name}-{results_name}/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_mode\n",
    "\n",
    "config_mode.init(mode_CAP)\n",
    "\n",
    "from fit_data_common import *\n",
    "from fit_data_list_maskers import *\n",
    "\n",
    "pl.style.use('seaborn-deep')\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from optim import *\n",
    "\n",
    "from rbf import RBFNet\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "if results_folder is None:\n",
    "    if Q10_distributed:\n",
    "        if I0_distributed:\n",
    "\n",
    "            results_folder=f'./results/fit{expe_name}-distrib/I0_distrib/'\n",
    "        else:\n",
    "            results_folder=f'./results/fit{expe_name}-distrib/'\n",
    "\n",
    "    else:\n",
    "        results_folder=f'./results/fit{expe_name}/'\n",
    "\n",
    "if write_results:\n",
    "    print(f'writting results in {results_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common to all CFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this section are coded in `fit_data_common.py` (also includes pre-processing of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_main_CAPs()\n",
    "plot_CAP_with_window()\n",
    "\n",
    "\n",
    "\n",
    "if expe_name == '1-22':\n",
    "    pass\n",
    "    plot_CAP_w_wo_filter()\n",
    "\n",
    "#NB: the plots below depend on the choice of the masking condition used for the estimation of ur\n",
    "plot_raw_excitation_deconv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrowband analysis\n",
    "\n",
    "The function below makes several plots: \n",
    " 1. plot of the CAP masking releases for the high-passed noise maskers\n",
    " 2. plot of the CAP masking releases by bands ($\\Delta CAP$ is computed as the CAP difference between two subsequent cut-off frequencies). First part (frequencies above 4 kHz)\n",
    " 3. Same as 2, second part (frequencies below 4kHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures_narrowband_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contributions by bands are deconvolved by a rough approximate of the unitary function (in example below, ur is the response to a notched-noise masker with a notch around 4 kHz). The deconvolution is done using Newton's optimization method, with a penalty similar to ridge regression to ensure stability. The excitation patterns are constrained to be non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures_narrowband_analysis_deconv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The delays for the peaks are retrieved from the last plot. The best fit with a power-law is searched (dog leg method).  \n",
    "$\\label{eq:latencies} CF(\\tau) = B (\\tau-t_0)_+^\\alpha$, parameters to fit: $t_0, B, \\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_estimated_latencies_deconv()\n",
    "plot_latencies_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for latencies after fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF specific\n",
    "\n",
    "#### 1. Prepare model for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First estimation of I/O masking curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_results and not(os.path.exists(results_folder)):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the masking releases for the notched noise maskers with varying atten. for the notch. The amount of masking is evaluated as the reduction of the CAP peak-to-peak amplitude. Serves as a helper to initialize I/O functions for the optimization process later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap=[]\n",
    "rms=[]\n",
    "masker_list=ntch_masker_lists[CF]  #, 'broadband_noise' \n",
    "masker_list=[st.replace('-', '_').replace('.json', '') for st in masker_list]\n",
    "\n",
    "\n",
    "reg_exp=ntch_regexps[CF]\n",
    "\n",
    "noise_rms=0\n",
    "\n",
    "for i, masker in enumerate(masker_list):\n",
    "    sig=capData.get_signal_by_name(masker)\n",
    "    if not(re.match(reg_exp, masker)):\n",
    "        continue\n",
    "    sig=process_signal(sig)\n",
    "    broadband_sig_trunc=process_signal(broadband2)\n",
    "    \n",
    "    #REF broadband\n",
    "    cap_amp=np.max(sig-broadband_sig_trunc)-np.min(sig-broadband_sig_trunc)\n",
    "    \n",
    "    #HACK\n",
    "    if '17dB' in masker:\n",
    "        cap_amp*=-1\n",
    "    cap.append(cap_amp)\n",
    "    \n",
    "    diff_sig_proc=gaussian_filter1d( sig-broadband_sig_trunc, gauss_sigma) #noise amp computed on filtered version\n",
    "    \n",
    "\n",
    "    noise_rms+=np.mean(diff_sig_proc[:ind0]**2)\n",
    "    pl.plot(t2*1e3, diff_sig_proc*1e3, label=masker)\n",
    "     \n",
    "noise_rms=np.sqrt(noise_rms/len(masker_list))\n",
    "print(f'noise rms: {noise_rms*1e3:.3f} μV')\n",
    "\n",
    "pl.xlabel('t (ms)')\n",
    "pl.ylabel('Amplitude difference (μV)')\n",
    "\n",
    "pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "pl.show()\n",
    "\n",
    "pl.figure(figsize=(8, 6))\n",
    "\n",
    "attns=-attns_arrays[CF]\n",
    "pl.plot(attns+20, cap, '+', label='max-min')  #20  REF\n",
    "\n",
    "pl.legend()\n",
    "pl.xlabel('Notch attenuation')\n",
    "pl.ylabel('Amplitude difference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigm=SigmoidIOFunc(0, 0)\n",
    "maskamount=1-(cap/np.amax(cap)) \n",
    "\n",
    "I_pts=I0+attns\n",
    "\n",
    "sigm.fit_data(I_pts, maskamount, constrained_at_Iref=True, Iref=I0-20)\n",
    "\n",
    "wb_cdf=WeibullCDF_IOFunc()\n",
    "\n",
    "wb_cdf.fit_data(I_pts, maskamount, constrained_at_Iref=True, Iref=I0-20)\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/maskamountCAP_{CF}.npz', I_pts=I_pts, maskamount=maskamount)\n",
    "    sigm.write_to_npz(f'{results_folder}/sigmIO_1st_estim_{CF}.npz')\n",
    "    wb_cdf.write_to_npz(f'{results_folder}/wbcfdIO_1st_estim_{CF}.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=np.linspace(-30, 25)\n",
    "\n",
    "fig=pl.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "pl.plot(I, sigm(torch.tensor(I))*100, label='fit sigmoid')\n",
    "pl.plot(I, wb_cdf(torch.tensor(I))*100, label='fit Weibull CDF')\n",
    "\n",
    "#plot after optim\n",
    "#pl.plot(I, wb_cdf2(torch.tensor(I)).clone().detach().numpy()*100, label='   (after optim.)', color='C1', linestyle='--')\n",
    "\n",
    "pl.xlabel('Power spectral density (dB)')\n",
    "\n",
    "pl.plot(I_pts, maskamount*100, '+', markersize=10, markeredgewidth=3, label='based on ΔCAP \\namplitude')\n",
    "\n",
    "pl.plot(I0-20, 100, '+', markersize=10, markeredgewidth=3, color='purple')\n",
    "\n",
    "pl.ylabel('Masking (%)')\n",
    "\n",
    "pl.ylim([0, 130])\n",
    "\n",
    "for key, spine in ax.spines.items():\n",
    "    spine.set_visible(True)\n",
    "    \n",
    "    spine.set_linewidth(1.3)\n",
    "    spine.set_edgecolor('black')\n",
    "\n",
    "\n",
    "ax.grid(which='minor')\n",
    "\n",
    "\n",
    "pl.legend()\n",
    "#pl.savefig('IO_func_fit.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting model for estimation of ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reg_exp=ntch_regexps[CF]  #previous method\n",
    "\n",
    "fln_list=ntch_masker_lists[CF]\n",
    "def get_regexp(fln_list):\n",
    "    masker_list=[st.replace('-', '_').replace('.json', '') for st in fln_list]\n",
    "    reg_exp=')|('.join(masker_list)\n",
    "    reg_exp='('+reg_exp+')'\n",
    "    return reg_exp\n",
    "    \n",
    "reg_exp=get_regexp(fln_list)\n",
    "\n",
    "ntch_maskerNames, ntch_maskingConds, ntch_signals =capData.get_batch_re(reg_exp)\n",
    "ntch_maskingConds.set_amp0_dB(I0)\n",
    "\n",
    "# pad maskers >12e3 to avoid issues with latencies (equivalent to taking the difference\n",
    "#  excitations of maskers - excitation 'broadband noise')\n",
    "ntch_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)\n",
    "ntch_maskingConds.pad_maskers2() #same thing for low freqs\n",
    "\n",
    "#gauss_sigma=(1e-4)/(t2[1]-t2[0])  #gauss_sigma defined in common.py\n",
    "ntch_signals_proc=process_signal2(ntch_signals, gauss_sigma=gauss_sigma)\n",
    "\n",
    "#shift latencies\n",
    "lat_model=lat\n",
    "\n",
    "#ref click\n",
    "lat_model=lat_shifted=PowerLawLatencies.shift(lat_model, 4.8e-3-1e-3)   #t0: (peak deconv ref:click)  + (t_click-1ms)\n",
    "lat_model=lat_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min=2500\n",
    "f_max=12000\n",
    "\n",
    "if E0_distributed:\n",
    "    with open('E0_params.json') as f:\n",
    "        params = json.load(f)\n",
    "        f_min=float(params['f_min'])\n",
    "        f_max=float(params['f_max'])\n",
    "\n",
    "\n",
    "if E0_distributed:\n",
    "    with open('E0_params.json') as f:\n",
    "        params = json.load(f)\n",
    "        m=int(params['m'])\n",
    "else:\n",
    "    m=400\n",
    "E0=1/2*np.ones((m,))\n",
    "\n",
    "pl.plot(np.linspace(f_min*1e-3, f_max*1e-3, m), E0)\n",
    "pl.xlabel('Frequency (kHz)')\n",
    "pl.ylabel('Init raw excitation')\n",
    "\n",
    "#ind for CF (can be useful later)\n",
    "ind_CF=int((CF-f_min)/(f_max-f_min)*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=ExcitationPatterns(t2, E0, use_bincount=use_bincount, bincount_fmin=f_min, bincount_fmax=f_max)  #no non-maskable part\n",
    "\n",
    "#NB: first model for estimation of ur, cte bandwith, fixed\n",
    "\n",
    "Q_10_0=2*(CF/1000)**0.5\n",
    "BW10_0=CF/Q_10_0\n",
    "\n",
    "#BW10_0Func=constant_BW10(BW10_0, requires_grad=False)   #constant BW\n",
    "\n",
    "#Q10 defined by the power law above\n",
    "BW10_0Func=Q10PowerLaw(2, 1000, 0.5, requires_grad=False)\n",
    "\n",
    "print(f'BW10 for first guess: {BW10_0Func(CF):.1f} Hz')\n",
    "\n",
    "E.set_masking_model(lat_model, BW10_0Func, ntch_maskingConds, wb_cdf, filter_model=filter_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimation of unitary response**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unitary response is estimated by deconvolution of the CAP masking releases $[\\Delta CAP(t)]_i$ for the notched-noise maskers with varying attenuation for the notch. A first guess for the masking release patterns is used (after optimization of the model parameters, a re-estimation of ur can be done with `load_wbcdf=True`, before a second optimisation). The UR is taken as the average of the deconvolved signals, weighted by the quadratic sum of $[\\Delta CAP(t)]_i$ for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signals from which ur is estimated\n",
    "\n",
    "fln_list=ntch_masker_lists[CF]+vbw_fln_lists[CF]\n",
    "\n",
    "reg_exp=get_regexp(fln_list)\n",
    "\n",
    "ur_estim_maskerNames, ur_estim_maskingConds, ur_estim_signals =capData.get_batch_re(reg_exp)\n",
    "ur_estim_maskingConds.set_amp0_dB(I0)\n",
    "\n",
    "gauss_sigma_deconv=2*gauss_sigma \n",
    "\n",
    "ur_estim_signals_proc=process_signal2(ur_estim_signals, gauss_sigma=gauss_sigma_deconv)\n",
    "ur_estim_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimation ur\n",
    "\n",
    "\n",
    "if load_wbcdf or I0_distributed:\n",
    "    wb_cdf=WeibullCDF_IOFunc.load_from_npz(f'{results_folder0}/wbcfdIO_{CF}.npz')\n",
    "\n",
    "E.set_masking_model(lat_model, BW10_0Func, ur_estim_maskingConds, wb_cdf, filter_model=filter_model)\n",
    "\n",
    "maskAmounts, excs = E.get_tensors() \n",
    "\n",
    "nb_steps=20\n",
    "alpha=np.linspace(0.5, 0.05, nb_steps)\n",
    "\n",
    "EPs_fft=np.fft.rfft(excs, axis=1)\n",
    "CAPs_fft=np.fft.rfft(ur_estim_signals_proc, axis=1)\n",
    "u1_mat=np.zeros_like(ur_estim_signals_proc)\n",
    "filter_mat  = (t2>7.5e-3)\n",
    "filter_mat=np.tile(filter_mat, (ur_estim_maskingConds.n_conditions, 1))\n",
    "\n",
    "weights=np.sqrt(np.sum(excs.clone().detach().numpy()**2, axis=1))\n",
    "for i in range(1, nb_steps+1):\n",
    "    du=deconv_newton_step(u1_mat, EPs_fft, CAPs_fft, eps_ridge=0)   \n",
    "    \n",
    "    u1_mat-=alpha[i-1]*du\n",
    "    #proj\n",
    "    u1_mat[filter_mat]=np.zeros_like(u1_mat[filter_mat])\n",
    "\n",
    "        \n",
    "    #weighted average\n",
    "    u1_mat_mean=np.average(u1_mat, axis=0, weights=weights)[None, :]\n",
    "    u1_mat=np.repeat(u1_mat_mean, ur_estim_maskingConds.n_conditions, axis=0)\n",
    "     \n",
    "    '''\n",
    "    for i in range(5):\n",
    "            pl.figure()\n",
    "            name=ntch_maskerNames[i]\n",
    "            pl.plot(u1_mat[i], label=name, color=f'C{i}')\n",
    "            #pl.plot( np.abs(EPs_fft[i]))\n",
    "            pl.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    '''\n",
    "    \n",
    "\n",
    "    if i==nb_steps:\n",
    "        pl.figure()\n",
    "        pl.title(f'Step {i} (deconv + proj)')\n",
    "        #pl.plot(t, u0, label='u0 (truth)')\n",
    "        #pl.plot(t2, u_temp, label='u0 (last save)')\n",
    "        pl.plot(t2, u1_mat[0], label='u0 (estimated)')\n",
    "        pl.legend()\n",
    "        #pl.savefig('ur_8kHz_Q395.svg')\n",
    "        pl.show()\n",
    "        u_temp=u1_mat[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting maskers and signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verious freqs. (for estimating E0)\n",
    "fln_list=vfreq_fln_lists[CF]\n",
    "\n",
    "reg_exp=get_regexp(fln_list)\n",
    "\n",
    "vfreq_maskerNames, vfreq_maskingConds, vfreq_signals =capData.get_batch_re(reg_exp)\n",
    "vfreq_signals_proc=process_signal2(vfreq_signals, gauss_sigma=gauss_sigma)\n",
    "vfreq_maskingConds.set_amp0_dB(I0)\n",
    "vfreq_maskingConds.pad_maskers(f_thr=11000, f_max=np.Inf)\n",
    "\n",
    "#various bws (for estimating Q10)\n",
    "fln_list=vbw_fln_lists[CF]\n",
    "\n",
    "reg_exp=get_regexp(fln_list)\n",
    "\n",
    "vbw_maskerNames, vbw_maskingConds, vbw_signals =capData.get_batch_re(reg_exp)\n",
    "vbw_signals_proc=process_signal2(vbw_signals, gauss_sigma=gauss_sigma)\n",
    "vbw_maskingConds.set_amp0_dB(I0)\n",
    "vbw_maskingConds.pad_maskers(f_thr=11000, f_max=1e5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fine-tuning of model parameters using gradient descent \n",
    "Updates (I/O curve, Q10, frequency weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of I/O curve (importing params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try more accurate estimation of i/o curve\n",
    "\n",
    "signals_proc=ntch_signals_proc\n",
    "maskingConds=ntch_maskingConds\n",
    "\n",
    "\n",
    "io_func = 'weibull' \n",
    "\n",
    "init_params_json=f'./init_params/{expe_name}/{CF}_init_params.json'\n",
    "if load_json_init_params and os.path.exists(init_params_json):\n",
    "    \n",
    "    with open(init_params_json) as f:\n",
    "        dic_params=json.load(f)\n",
    "        k_cdf=float(dic_params['k'])\n",
    "        I0_cdf=float(dic_params['I0'])\n",
    "        scale_cdf=float(dic_params['scale'])\n",
    "else:\n",
    "    I0_cdf=-20.\n",
    "    k_cdf=5.\n",
    "    scale_cdf=30.\n",
    "    \n",
    "    \n",
    "    \n",
    "wb_cdf2=WeibullCDF_IOFunc(I0=I0_cdf,\n",
    "    scale=scale_cdf,\n",
    "    k=k_cdf,\n",
    "    mmax=1.,\n",
    "    requires_grad=True,\n",
    "    constrained_at_Iref=True,\n",
    "    Iref=I0-20)\n",
    "\n",
    "E2=ExcitationPatterns.copyRaw(E, requires_grad=True)\n",
    "if Q10_distributed or E0_distributed:   \n",
    "    #init group\n",
    "    if not(dist.is_initialized()):\n",
    "        dist.init_process_group(backend, init_method='tcp://127.0.0.1:1234', world_size=n_workers, rank=rank, \n",
    "                                timeout=datetime.timedelta(0, 80))  \n",
    "    \n",
    "    if Q10_distributed:\n",
    "        Q10rbf=Q10RBFNet.create_from_jsonfile('RBF_params.json')\n",
    "        #update weights (have to be sent by main process)\n",
    "        Q10rbf.update_weights()\n",
    "        BW10_0TestFunc=Q10RBFNet_BW10(Q10rbf)\n",
    "    \n",
    "else:\n",
    "    BW10_0TestFunc=constant_BW10(BW10_0, requires_grad=True)\n",
    "if I0_distributed:\n",
    "    I0_rbf=RBFNet.create_from_jsonfile('RBF_I0_params.json')\n",
    "    wb_cdf2.set_I0_w_RBFNet(I0_rbf, plus_lambda=plus_lambda)\n",
    "    #update weights (have to be sent by main process)\n",
    "    I0_rbf.update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting gradient step sizes for few runs of optimization loop (to update I/O curve only first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=30\n",
    "alpha_Q10=3e7\n",
    "\n",
    "n_dim=7 #projection of gradient on n_dim first harmomics for E_0 (Fourier basis)\n",
    "\n",
    "if io_func=='weibull':\n",
    "    alpha_dic={wb_cdf2.scale: alpha, wb_cdf2.k: 10*alpha}\n",
    "    \n",
    "    if I0_distributed:\n",
    "        alpha_dic[wb_cdf2.rbfNet.l2.weight]=0.005*alpha\n",
    "    else:\n",
    "        alpha_dic[wb_cdf2.I0]=10*alpha\n",
    "else:\n",
    "    alpha_dic={sigm2.mu: 0.01*alpha, sigm2.a: 0.005*alpha}\n",
    "\n",
    "#alpha_dic[BW10_0TestFunc.BW_10]=alpha\n",
    "#alpha_dic[E2.E0_maskable]=0.1*alpha  #/!| with sum_grad_E0 set to True #previous method to modify E0 amp\n",
    "alpha_dic[E2.E0_maskable_amp]=0.1*alpha  \n",
    "\n",
    "\n",
    "alpha_dic_Q10={}\n",
    "\n",
    "if Q10_distributed:\n",
    "    alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]=0.05*alpha\n",
    "else:\n",
    "    alpha_dic_Q10[BW10_0TestFunc.BW_10]=alpha_Q10 #cte bw\n",
    "    \n",
    "\n",
    "\n",
    "alpha_dic_E0={E2.E0_maskable: 0.2*alpha}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first loop of 50 gradient descent steps is performed to fit the I/O function on the responses for the notched-noise maskers with a varying attenuation for the notch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first optim I/O func (if not I0 distributed or load_wbcdf)\n",
    "\n",
    "nb_stepsIO=50\n",
    "\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "if not(I0_distributed or load_wbcdf):\n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "                nb_steps=nb_stepsIO, #sum_grad_E0=True, \n",
    "                                plot_masking_I0_graph=True,\n",
    "               step_plots=5)\n",
    "else:\n",
    "    #import params (k, scale)\n",
    "    \n",
    "    wb_cdf_temp=WeibullCDF_IOFunc.load_from_npz(f'{results_folder0}/wbcfdIO_{CF}.npz')\n",
    "    if not(I0_distributed):\n",
    "        wb_cdf2.I0.data=wb_cdf_temp.I0.data\n",
    "    wb_cdf2.k.data=wb_cdf_temp.k.data\n",
    "    wb_cdf2.scale.data=wb_cdf_temp.scale.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unitary response is normalized (so that it is comparable across CFs, i.e. different optimization nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convention: normalize so that amp=0.0001 (/20 factor compared to others expes)\n",
    "#norm_factor=0.0001/(np.amax(u1_mat[0])-np.amin(u1_mat[0]) ) #normalization peak-to-peak\n",
    "norm_factor=0.00005/(-np.amin(u1_mat[0]) ) #normalization N1\n",
    "\n",
    "u1_mat*=norm_factor\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/ur_{CF}.npz', t2=t2, ur=u1_mat[0])\n",
    "\n",
    "if not(E0_distributed):\n",
    "    E2.E0_maskable.data=E2.E0_maskable/norm_factor\n",
    "else:\n",
    "    #send norm_factor to main node\n",
    "    send_norm_factor_hand=dist.isend(torch.tensor(norm_factor, dtype=torch.float64),0, tag=99)\n",
    "    send_norm_factor_hand.wait() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting gradient step sizes for main loop (loads params from external json file).  \n",
    "Total number of steps is 3 x n_it x nb_steps (=3 x 100 x 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if io_func=='weibull':\n",
    "    if not(I0_distributed):\n",
    "        alpha_dic[wb_cdf2.I0]=0.5*alpha\n",
    "    alpha_dic[wb_cdf2.scale]= 0.05*alpha\n",
    "    alpha_dic[wb_cdf2.k]= 0.5*alpha    \n",
    "\n",
    "n_it=100 \n",
    "nb_steps=3 \n",
    "\n",
    "if load_json_optim_params:\n",
    "    if os.path.exists(f'optim_params_{expe_name}.json'):\n",
    "        optim_params_filename=f'optim_params_{expe_name}.json'\n",
    "    else:\n",
    "        optim_params_filename='optim_params.json'\n",
    "    with open(optim_params_filename) as f:\n",
    "        dic_params=json.load(f)\n",
    "    \n",
    "    n_it=dic_params['n_it']\n",
    "    nb_steps=dic_params['nb_steps']\n",
    "    n_dim=dic_params['n_dim']\n",
    "    step_values=dic_params['alpha']\n",
    "    if io_func=='weibull':\n",
    "        if I0_distributed:\n",
    "            alpha_dic[wb_cdf2.rbfNet.l2.weight]=float(step_values['I0RBFweights'])\n",
    "        else:\n",
    "            alpha_dic[wb_cdf2.I0]=float(step_values['I0'])\n",
    "        alpha_dic[wb_cdf2.scale]= float(step_values['scale'])\n",
    "        alpha_dic[wb_cdf2.k]= float(step_values['k'] )\n",
    "    else:\n",
    "        alpha_dic[sigm2.mu]= float(step_values['sigm_mu'])\n",
    "        alpha_dic[sigm2.a]= float(step_values['sigm_a'])\n",
    "    alpha_dic[E2.E0_maskable_amp]=float(step_values['E0_amp'])\n",
    "        \n",
    "\n",
    "    if Q10_distributed:\n",
    "        alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]=float(step_values['Q10RBFweights'])\n",
    "    else:\n",
    "        alpha_dic_Q10[BW10_0TestFunc.BW_10]=float(step_values['Q10']) #cte bw\n",
    "                                  \n",
    "    \n",
    "    alpha_dic_E0[E2.E0_maskable]=float(step_values['E0'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optim (main loop)**\n",
    "\n",
    "The model parameters are fine-tuned using an alternate gradient scheme.\n",
    " 1. Update of the weights R0 (E0 in the code). Notched noise maskers with a notch present on a broad range of frequencies around CF are used for the computation of gradients. 3 steps\n",
    " 2.  Update of the weights I/0 function (computation of gradients over notched noise maskers with various notch attenations). 3 steps\n",
    " 3.  Update of Q_10 (notched noise maskers with varying notch width)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Debug plot gradients\n",
    "\n",
    "E2.E0_maskable_amp.data=E0[ind_CF]/E2.E0_maskable[ind_CF]*1/norm_factor\n",
    "\n",
    "pl.figure(figsize=(6, 4))\n",
    "\n",
    "#I/O Func (+ amp E0)\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "            nb_steps=1, #sum_grad_E0=True, \n",
    "            plot_E0_graph=False, plot_masking_I0_graph=False,\n",
    "            plot_Q10=False, debug_grad_excs=True,\n",
    "                                       fc_ref_Q10=CF, \n",
    "           step_plots=1) #axes=axes, ind_plots=ind_plots, step0=(3*i+1)*nb_steps, tot_steps=tot_steps, I0_distributed=I0_distributed\n",
    "pl.xlim([6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tot_steps=3*n_it*nb_steps\n",
    "\n",
    "errs=[]\n",
    "errs_total=[]\n",
    "\n",
    "pl.figure(figsize=(6, 12))\n",
    "\n",
    "for i in range(n_it):\n",
    "    \n",
    "    if Q10_distributed or E0_distributed or I0_distributed: #informs the main node that optim is still in process\n",
    "        optim_done_hand=dist.isend(torch.tensor(nb_steps, dtype=torch.int32),0, tag=16)\n",
    "        optim_done_hand.wait()\n",
    "    \n",
    "    if E0_distributed:  #update E0\n",
    "        hand = dist.irecv(E2.E0_maskable, src=0, tag=8)\n",
    "        hand.wait()\n",
    "    if Q10_distributed:\n",
    "        Q10rbf.update_weights()\n",
    "    \n",
    "    if I0_distributed:\n",
    "        I0_rbf.update_weights()\n",
    "       \n",
    "    #E0\n",
    "    if i==0:\n",
    "        #try to have E0_amp and E0 values consistent with each other\n",
    "        E2.E0_maskable_amp.data=E0[ind_CF]/E2.E0_maskable[ind_CF]*1/norm_factor\n",
    "        \n",
    "         #update E0_amp if E0_distributed for a few steps before anything else\n",
    "        if E0_distributed:\n",
    "            if io_func=='weibull':\n",
    "                E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "            else:\n",
    "                E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "            optim_steps(E2, u1_mat[0], signals_proc, {E2.E0_maskable_amp:alpha_dic[E2.E0_maskable_amp]}, \n",
    "                nb_steps=nb_steps*5)\n",
    "        \n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "    if i==0:\n",
    "        axes=None\n",
    "        ind_plots=None\n",
    "        \n",
    "\n",
    "        \n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], vfreq_signals_proc, alpha_dic_E0, \n",
    "        nb_steps=nb_steps, \n",
    "        n_dim_E0=n_dim, \n",
    "        E0_distributed=E0_distributed,                        \n",
    "        plot_E0_graph=True, plot_E0_amp_graph=True, plot_masking_I0_graph=True,\n",
    "        plot_Q10=True, fc_ref_Q10=CF, step_plots=5, axes=axes, ind_plots=ind_plots, \n",
    "        step0=(3*i)*nb_steps, tot_steps=tot_steps) \n",
    "        \n",
    "    err0=err_list[-1]\n",
    "\n",
    "    #I/O Func (+ amp E0)\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], signals_proc, alpha_dic, \n",
    "                nb_steps=nb_steps, \n",
    "                plot_E0_graph=True, plot_masking_I0_graph=True,\n",
    "                plot_Q10=True, fc_ref_Q10=CF,\n",
    "               step_plots=5, axes=axes, ind_plots=ind_plots, step0=(3*i+1)*nb_steps,\n",
    "                 tot_steps=tot_steps, I0_distributed=I0_distributed)\n",
    "    err1=err_list[-1]\n",
    "\n",
    "#     #Q10\n",
    "    \n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm, filter_model=filter_model)\n",
    "\n",
    "    axes, ind_plots, err_list=optim_steps(E2, u1_mat[0], vbw_signals_proc, alpha_dic_Q10, \n",
    "            nb_steps=nb_steps, sum_grad_E0=True, \n",
    "            plot_E0_graph=True, plot_masking_I0_graph=True,\n",
    "            plot_Q10=True, fc_ref_Q10=CF,\n",
    "           step_plots=5, axes=axes, ind_plots=ind_plots, step0=(3*i+2)*nb_steps,\n",
    "               tot_steps=tot_steps, \n",
    "               Q10_distributed=Q10_distributed)\n",
    "    err2=err_list[-1]\n",
    "    err_sum=(err1+err2)\n",
    "    errs.append(err_sum.detach().numpy())  #errors are summed only on notched noise maskers (update I/O curve and Q10)\n",
    "    #nb: possible duplicates\n",
    "    errs_total.append( (err0+err1+err2))\n",
    "\n",
    "    \n",
    "if Q10_distributed or E0_distributed or I0_distributed: #informs the main node that optim is done\n",
    "    optim_done_hand=dist.isend(torch.tensor(0, dtype=torch.int32),0, tag=16)\n",
    "        \n",
    "pl.tight_layout()\n",
    "if save_figs:\n",
    "    pl.savefig(f'fitdata{CF}_optim_steps.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also `Fit_data_synthesis.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_2=np.sum(vbw_signals_proc**2)+np.sum(ntch_signals_proc**2)  \n",
    "rms_2b=rms_2+np.sum(vfreq_signals_proc**2)  \n",
    "\n",
    "pl.figure()\n",
    "pl.plot(np.arange(len(errs)), errs/rms_2*100, label='notched noise maskers')\n",
    "\n",
    "pl.xlabel('Iterations')\n",
    "pl.ylabel('Error (% variance)')\n",
    "\n",
    "pl.legend()\n",
    "\n",
    "if save_figs:\n",
    "    pl.savefig(f'fitdata{CF}_optim_steps_err.svg')\n",
    "pl.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_results:\n",
    "    if io_func=='weibull':\n",
    "        if I0_distributed:\n",
    "            I0_=wb_cdf2.rbfNet(torch.tensor([CF]))\n",
    "            if plus_lambda:\n",
    "                I0_-=wb_cdf2.scale\n",
    "            wb_cdf2.I0=I0_[0]\n",
    "        wb_cdf2.write_to_npz(f'{results_folder}/wbcfdIO_{CF}.npz')\n",
    "    else:\n",
    "        sigm2.write_to_npz(f'{results_folder}/sigmIO_{CF}.npz')    \n",
    "    \n",
    "    if isinstance(lat_model, SingleLatency):\n",
    "        np.savez(f'{results_folder}/E0_{CF}.npz', f=lat_model.get_f_linspace(len(t2)).detach().numpy(),\n",
    "                E0=E2.E0_maskable.detach().numpy(), lat=lat_model.t0, E0_amp=E2.E0_maskable_amp.detach().numpy())\n",
    "    else:\n",
    "        if use_bincount: \n",
    "            np.savez(f'{results_folder}/E0_{CF}.npz', f=E2.bincount_f.detach().numpy(),\n",
    "                E0=E2.E0_maskable.detach().numpy(), E0_amp=E2.E0_maskable_amp.detach().numpy())\n",
    "            \n",
    "       \n",
    "        #save lat model\n",
    "        lat_model.write_to_npz(f'{results_folder}/lat_{CF}.npz') #Note: normally lat does not depend on CF but it could\n",
    "        \n",
    "    if Q10_distributed:\n",
    "        pass\n",
    "\n",
    "    Q10optim= CF/E2.bw10Func(torch.tensor(CF, dtype=torch.float32))\n",
    "    np.save(f'{results_folder}/Q10optim_{CF}.npy',\n",
    "            Q10optim.detach().numpy() )\n",
    "    \n",
    "    \n",
    "    #write params\n",
    "    \n",
    "    json_data={}\n",
    "    json_data[\"n_it\"]=n_it\n",
    "    json_data[\"nb_steps\"]=nb_steps\n",
    "    json_data[\"tot_steps\"]=tot_steps\n",
    "\n",
    "    alpha=30\n",
    "    alpha_Q10=3e7\n",
    "\n",
    "\n",
    "    #for estimation of E0\n",
    "    json_data[\"n_dim\"]=n_dim \n",
    "\n",
    "    if io_func=='weibull':\n",
    "        json_data_alpha={\"scale\": alpha_dic[wb_cdf2.scale],\n",
    "                         \"k\": alpha_dic[wb_cdf2.k]}\n",
    "        if I0_distributed:\n",
    "            json_data_alpha[\"I0_rbf_weights\"]=alpha_dic[wb_cdf2.rbfNet.l2.weight]\n",
    "        else:\n",
    "            json_data_alpha[\"I0\"]=alpha_dic[wb_cdf2.I0]\n",
    "    else:\n",
    "        json_data_alpha={\"mu\": alpha_dic[sigm2.mu], \"a\": alpha_dic[sigm2.a]}\n",
    "\n",
    "        \n",
    "    json_data_alpha[\"E0_amp\"]=alpha_dic[E2.E0_maskable_amp]\n",
    "\n",
    "    \n",
    "    json_data[\"Q10_distributed\"]=Q10_distributed\n",
    "    \n",
    "    \n",
    "    json_data[\"E0_distributed\"]=E0_distributed\n",
    "    \n",
    "    \n",
    "    json_data[\"I0_distributed\"]=I0_distributed\n",
    "    \n",
    "    if Q10_distributed:\n",
    "        json_data_alpha[\"Q10RBFweights\"]=alpha_dic_Q10[BW10_0TestFunc.Q10RBFnet.l2.weight]\n",
    "    else:\n",
    "        json_data_alpha[\"Q10\"]= alpha_dic_Q10[BW10_0TestFunc.BW_10] #cte bw\n",
    "\n",
    "\n",
    "\n",
    "    json_data_alpha[\"E0\"]=alpha_dic_E0[E2.E0_maskable]\n",
    "    \n",
    "    json_data[\"alpha\"]=json_data_alpha\n",
    "    \n",
    "    \n",
    "    with open(f'{results_folder}/optim_params_{CF}.json', 'w') as outfile:\n",
    "        json.dump(json_data, outfile, indent=4)\n",
    "\n",
    "    np.save(f'{results_folder}/err_list_{CF}.npy',\n",
    "            np.array(errs)/rms_2 )\n",
    "    \n",
    "    sig_rms=rms_2*1/ (vbw_maskingConds.n_conditions+ntch_maskingConds.n_conditions)*1/np.shape(vbw_signals_proc**2)[1]\n",
    "    sig_rms=np.sqrt(sig_rms)\n",
    "    np.savez(f'{results_folder}/err_list_{CF}.npz', sum_sq_err=np.array(errs), sum_sq_sig=rms_2, \n",
    "            noise_rms=noise_rms, sig_rms=sig_rms, snr=sig_rms/noise_rms)  #+info noise level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative for errors: computes errors on smaller interval, inside Tukey window (easier to compare to noise level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_bis=t0+alpha_tukey/2*(t1-t0)  #interval inside 100% for tukey window\n",
    "t1_bis=t1-alpha_tukey/2*(t1-t0)\n",
    "\n",
    "#Tukey window is applied to truncated signal -> needs to correct times\n",
    "t0_bis+=float(E2.t[0])\n",
    "t1_bis+=float(E2.t[0])\n",
    "\n",
    "\n",
    "print(f't0_bis: {t0_bis*1e3:.3f} ms, t1_bis: {t1_bis*1e3:.3f} ms')\n",
    "\n",
    "\n",
    "#ntch_maskingConds\n",
    "u1=u1_mat[0]    \n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "\n",
    "sq_err_ntch, sq_sig_ntch =get_sq_err_CAPs(E2, u1, ntch_signals_proc, t0_bis, t1_bis)\n",
    "\n",
    "\n",
    "#vbw_maskingConds\n",
    "\n",
    "\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    \n",
    "    \n",
    "sq_err_vbw, sq_sig_vbw=get_sq_err_CAPs(E2, u1, vbw_signals_proc, t0_bis, t1_bis)\n",
    "\n",
    "\n",
    "#vfreq_maskingConds\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "sq_err_vfreq, sq_sig_vfreq =get_sq_err_CAPs(E2, u1, vfreq_signals_proc, t0_bis, t1_bis)\n",
    "sig_rms_vfreq=np.sqrt(np.sum(sq_sig_vfreq)/ vfreq_maskingConds.n_conditions )\n",
    "err_rms_vfreq=np.sqrt(np.sum(sq_err_vfreq)/ vfreq_maskingConds.n_conditions )\n",
    "\n",
    "\n",
    "sig_rms2_list=[]\n",
    "errs2_rms_list=[]\n",
    "print('On 100% Tukey window: ')\n",
    "for (eps1, eps2, supp_text) in [(1, 0, '(various notch widths)'), \n",
    "                                (0, 1, '(various notch atten)'),\n",
    "                                (1,1, 'overall')]:\n",
    "    sig_rms2=(eps1*np.sum(sq_sig_vbw)+eps2*np.sum(sq_sig_ntch))*1/ (eps1*vbw_maskingConds.n_conditions+eps2*ntch_maskingConds.n_conditions)\n",
    "    sig_rms2=np.sqrt(sig_rms2)\n",
    "    sig_rms2_list.append(sig_rms2)\n",
    "    \n",
    "    errs2_rms=(eps1*np.sum(sq_err_vbw)+eps2*np.sum(sq_err_ntch))*1/ (eps1*vbw_maskingConds.n_conditions+eps2*ntch_maskingConds.n_conditions)\n",
    "    errs2_rms=np.sqrt(errs2_rms)\n",
    "    errs2_rms_list.append(errs2_rms)\n",
    "\n",
    "    print(f'  signal RMS {supp_text} : {sig_rms2*1e3:.3f}  μV, mean error (RMS): {errs2_rms*1e3:.3f}  μV (estimated noise level: {noise_rms*1e3:.3f}  μV)')\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/err_list_{CF}_inside_window.npz',  \n",
    "            noise_rms=noise_rms, sig_rms=sig_rms2_list[2], sig_rms_ntch=sig_rms2_list[1],\n",
    "            sig_rms_vbw=sig_rms2_list[0],  err_rms=errs2_rms_list[2], err_rms_ntch=errs2_rms_list[1],\n",
    "            err_rms_vbw=errs2_rms_list[0],\n",
    "            sig_rms_vfreq=sig_rms_vfreq, err_rms_vfreq=err_rms_vfreq)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plots of $\\Delta CAP$ estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for excitation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #various notch atten\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    #various notch widths\n",
    "    \n",
    "#     if io_func=='weibull':\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "#     else:\n",
    "#         E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "    pl.figure(figsize=(10,20))\n",
    "    plotExcitationPatterns(E2, plot_raw_excitation=True) # ylim_top=1\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for notched-noise maskers at various frequencies for the notch (tests frequency weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "with torch.no_grad():\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc, vfreq_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    u1=u1_mat[0]\n",
    "    pl.figure(figsize=(12,20))\n",
    "    ax_list=plotSimulatedCAPs(E2, u1, max_plots=10, sig_exc=sig_exc_plot)\n",
    "    plotSimulatedCAPs(E2, CAParray=vfreq_signals_proc, axlist=ax_list, max_plots=10, plot_excitations=False, plotargs={\"color\":'C2'})\n",
    "    \n",
    "    \n",
    "    if save_figs:\n",
    "        pl.savefig(f'fitdata{CF}_vfreq_maskConds.svg')\n",
    "    \n",
    "    pl.plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for notched-noise maskers with various notch attenuations (tests I/O functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@interact_manual(I0=(-30, 0), scale=(10, 50), k=(0.5, 15), plot_only_learned=False)   #only works for weibull CDF\n",
    "def plot_v_attn_notch(I0, scale, k, plot_only_learned):\n",
    "    print('After learning: ')\n",
    "    print(wb_cdf2)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \n",
    "        if not(plot_only_learned):\n",
    "            wb_cdf_temp=WeibullCDF_IOFunc(constrained_at_Iref=True, Iref=wb_cdf2._Iref, I0=I0, \n",
    "                                      scale=scale, k=k)\n",
    "                \n",
    "            I=torch.linspace(-30, 30, 50)\n",
    "            pl.figure()\n",
    "\n",
    "            pl.plot(I, wb_cdf2(I))\n",
    "\n",
    "            pl.plot(I, wb_cdf_temp(I))\n",
    "            pl.xlim([-20, 30])\n",
    "            pl.title('Masking IO Function')\n",
    "            pl.xlabel('Power spectral density (dB)')\n",
    "            pl.show()\n",
    "        \n",
    "        if io_func=='weibull':\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "        else:\n",
    "            E2.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, sigm2, filter_model=filter_model)\n",
    "\n",
    "        u1=u1_mat[0]\n",
    "        pl.figure(figsize=(12,20))\n",
    "        ax_list=plotSimulatedCAPs(E2, u1, ylim=[-15, 15], max_plots=10, sig_exc=sig_exc_plot)\n",
    "\n",
    "        \n",
    "        if io_func=='weibull' and not plot_only_learned:\n",
    "        \n",
    "            E_temp=ExcitationPatterns.copyRaw(E2)\n",
    "            E_temp.set_masking_model(lat_model, BW10_0TestFunc, ntch_maskingConds, wb_cdf_temp, filter_model=filter_model)\n",
    "        \n",
    "            plotSimulatedCAPs(E_temp, u1, axlist=ax_list, max_plots=10, sig_exc=sig_exc_plot)\n",
    "\n",
    "        plotSimulatedCAPs(E2, CAParray=ntch_signals_proc, axlist=ax_list, max_plots=10, \n",
    "                          plot_excitations=False, plotargs={\"color\":'C2'})\n",
    "    if save_figs:\n",
    "        pl.savefig(f'fitdata{CF}_ntch_maskConds.svg')\n",
    "    pl.plot()\n",
    "    \n",
    "plot_v_attn_notch(0, 15, 5, True) #hack learned curve (random params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for notched-noise maskers  with various notch widths (tests $Q_{10}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "u1=u1_mat[0]\n",
    "pl.figure(figsize=(10,14))\n",
    "if io_func=='weibull':\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "else:\n",
    "    E2.set_masking_model(lat_model, BW10_0TestFunc, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "    \n",
    "    \n",
    "with torch.no_grad():\n",
    "    ax_list=plotSimulatedCAPs(E2, u1, ylim=[-10, 10], sig_exc=sig_exc_plot)\n",
    "    plotSimulatedCAPs(E2, CAParray=vbw_signals_proc, axlist=ax_list, plot_excitations=False, plotargs={\"color\":'C2'})\n",
    "\n",
    "if save_figs:\n",
    "    pl.savefig(f'fitdata{CF}_vbw_maskConds.svg')    \n",
    "pl.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for $Q_{10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_arr=np.linspace(500, 5000, num= ((4000-500)//50+1) )\n",
    "sigs_ref=vbw_signals_proc\n",
    "errs=[]\n",
    "for bw in bw_arr:\n",
    "\n",
    "    BW10_0TestFunc2=constant_BW10(bw, requires_grad=False)\n",
    "\n",
    "\n",
    "    if io_func=='weibull':\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc2, vbw_maskingConds, wb_cdf2, filter_model=filter_model)\n",
    "    else:\n",
    "        E2.set_masking_model(lat_model, BW10_0TestFunc2, vbw_maskingConds, sigm2, filter_model=filter_model)\n",
    "    excs = E2.get_tensor() \n",
    "    maskingConditions = E2.maskingConditions\n",
    "    err=0\n",
    "    for i, exc in zip(range(maskingConditions.n_conditions), excs):\n",
    "        exc_np = exc.detach().numpy()\n",
    "        CAP=np.convolve(exc_np, u1, mode='full')\n",
    "        t=E.t.numpy()\n",
    "        CAP=CAP[0:len(E2.t)]\n",
    "        err+=np.mean( (CAP-sigs_ref[i])**2)\n",
    "    errs.append(err/maskingConditions.n_conditions*1e6)\n",
    "    \n",
    "pl.plot(bw_arr, np.sqrt(errs))\n",
    "pl.xlabel('BW10 model (Hz)')\n",
    "\n",
    "pl.ylabel('Mean error (μV)')\n",
    "\n",
    "if save_figs:\n",
    "    pl.savefig(f'fitdata{CF}_BW10_errs.svg')\n",
    "\n",
    "\n",
    "ind_min=np.argmin(errs)\n",
    "print(f'estimated bw10: {bw_arr[ind_min]:.0f} Hz')\n",
    "\n",
    "if write_results:\n",
    "    np.savez(f'{results_folder}/Q10gridsearch_{CF}.npz', bw=bw_arr, errs=errs, bw10_est=bw_arr[ind_min])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
